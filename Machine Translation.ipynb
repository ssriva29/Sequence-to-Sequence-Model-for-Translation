{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment 3 IDS 576.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4o7LvsnUYyC"
      },
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_z5e_WEdDR"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdHtw7J6PBCH"
      },
      "source": [
        "Importing IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDNNmwH4nYsO"
      },
      "source": [
        "##Q 1.1) A Build a Markov (n-gram) language model\n",
        "We have a markov language model with trigram. We have used Torch test to retrieve the IMDB dataset and custom function is used to generate trigrams which is passed as the preprocessing argument in the data.field function and transforms each review into trigrams. Also, freqs function is used to get the count of each trigram. Data is further manipulated to calculate probabilties and predictions are made my smapling these probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMt-CtdHnS-R"
      },
      "source": [
        "#Generate trigrams by passing in the data\n",
        "def generate_trigrams(x):\n",
        "    res = []\n",
        "    n_grams = list(zip(*[x[i:] for i in range(3)]))\n",
        "    for n_gram in n_grams:\n",
        "        res.append(' '.join(n_gram))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShT1JoG2aKDJ",
        "outputId": "ab9218f1-f7c9-43b8-8352-6e6f294a8e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Importing and preprocessing the data to convert into trigrams.\n",
        "SEED = 12345\n",
        "from torchtext import datasets\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "#importing imdb daaset\n",
        "TEXT = data.Field(tokenize='spacy',preprocessing=generate_trigrams)\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)\n",
        "print(vars(train_data.examples[0]))\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED),split_ratio=0.9)\n",
        "TEXT.build_vocab(train_data)\n",
        "dictionary = dict(TEXT.vocab.freqs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz:   0%|          | 295k/84.1M [00:00<00:32, 2.57MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 51.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'text': ['I have the', 'have the entire', 'the entire Weissmuller', 'entire Weissmuller Tarzan', 'Weissmuller Tarzan series', 'Tarzan series on', 'series on DVD', 'on DVD (', 'DVD ( fully', '( fully restored', 'fully restored editions', 'restored editions )', 'editions ) &', ') & I', '& I never', 'I never tire', 'never tire of', 'tire of watching', 'of watching them', 'watching them .', 'them . My', '. My personal', 'My personal favorite', 'personal favorite is', 'favorite is \"', 'is \" Tarzan', '\" Tarzan and', 'Tarzan and His', 'and His Mate', 'His Mate \"', 'Mate \" ,', '\" , due', ', due entirely', 'due entirely (', 'entirely ( well', '( well almost', 'well almost entirely', 'almost entirely )', 'entirely ) to', ') to Maureen', \"to Maureen O'Sullivan\", \"Maureen O'Sullivan 's\", \"O'Sullivan 's costume\", \"'s costume and\", 'costume and the', 'and the occasional', 'the occasional flashes', 'occasional flashes of', 'flashes of her', 'of her genital', 'her genital area', 'genital area beneath', 'area beneath that', 'beneath that leather', 'that leather flap', 'leather flap hanging', 'flap hanging in', 'hanging in front', 'in front .', 'front . Before', '. Before anyone', 'Before anyone claims', 'anyone claims that', 'claims that A', 'that A -', 'A - It', '- It was', \"It was n't\", \"was n't really\", \"n't really her\", 'really her ,', 'her , or', ', or B', 'or B -', 'B - It', '- It was', \"It was n't\", \"was n't really\", \"n't really what\", 'really what it', 'what it looks', 'it looks like', 'looks like ,', 'like , let', ', let me', 'let me say', 'me say that', 'say that I', 'that I have', 'I have watched', 'have watched it', 'watched it numerous', 'it numerous time', 'numerous time ,', 'time , in', ', in high', 'in high zoom', 'high zoom mode', 'zoom mode ,', 'mode , and', ', and trust', 'and trust me', 'trust me ...', 'me ... it', '... it IS', 'it IS her', 'IS her ,', 'her , AND', ', AND she', 'AND she is', 'she is completely', 'is completely naked', 'completely naked underneath', 'naked underneath that', 'underneath that costume', 'that costume ...', 'costume ... several', '... several times', 'several times ,', 'times , especially', ', especially during', 'especially during the', 'during the lion', 'the lion attack', 'lion attack at', 'attack at the', 'at the end', 'the end ,', 'end , careful', ', careful viewing', 'careful viewing in', 'viewing in slow', 'in slow motion', 'slow motion and', 'motion and maximum', 'and maximum zoom', 'maximum zoom will', 'zoom will reveal', 'will reveal that', 'reveal that she', 'that she was', 'she was shaved', 'was shaved except', 'shaved except for', 'except for a', 'for a tiny', 'a tiny patch', 'tiny patch of', 'patch of dark', 'of dark hair', 'dark hair covering', 'hair covering her', 'covering her labia', 'her labia ...', 'labia ... There', '... There is', 'There is NO', 'is NO mistake', 'NO mistake about', 'mistake about that', 'about that at', 'that at all', 'at all .', 'all . As', '. As to', 'As to the', 'to the swimming', 'the swimming scene', 'swimming scene being', 'scene being a', 'being a body', 'a body double', 'body double in', 'double in a', 'in a \"', 'a \" skin', '\" skin \"', 'skin \" suit', '\" suit ,', 'suit , yes', ', yes ,', 'yes , it', ', it is', 'it is a', 'is a double', 'a double ,', 'double , BUT', ', BUT she', 'BUT she is', 'she is NOT', 'is NOT wearing', 'NOT wearing any', 'wearing any \"', 'any \" skin', '\" skin \"', 'skin \" suit', '\" suit or', 'suit or anything', 'or anything else', 'anything else ...', 'else ... again', '... again ,', 'again , slow', ', slow motion', 'slow motion and', 'motion and maximum', 'and maximum zoom', 'maximum zoom shows', 'zoom shows everything', 'shows everything to', 'everything to those', 'to those who', 'those who want', 'who want to', 'want to see', 'to see it', 'see it .', 'it . Now', '. Now ,', 'Now , that', ', that controversy', 'that controversy out', 'controversy out of', 'out of the', 'of the way', 'the way ,', 'way , let', \", let 's\", \"let 's move\", \"'s move on\", 'move on the', 'on the actual', 'the actual movie', 'actual movie ...', 'movie ... I', '... I thought', 'I thought the', 'thought the script', 'the script was', 'script was really', 'was really well', 'really well thought', 'well thought out', 'thought out and', 'out and written', 'and written tightly', 'written tightly ...', 'tightly ... The', '... The action', 'The action sequences', 'action sequences were', 'sequences were simply', 'were simply great', 'simply great ,', 'great , although', ', although it', 'although it is', 'it is obviously', 'is obviously a', 'obviously a stuntman', 'a stuntman riding', 'stuntman riding the', 'riding the rhino', 'the rhino ,', 'rhino , Weissmuller', ', Weissmuller actually', 'Weissmuller actually wrestles', 'actually wrestles the', 'wrestles the big', 'the big male', 'big male lion', 'male lion ...', 'lion ... The', '... The use', 'The use of', 'use of background', 'of background shots', 'background shots that', 'shots that were', 'that were second', 'were second unit', 'second unit stuff', 'unit stuff from', 'stuff from Africa', 'from Africa is', 'Africa is very', 'is very well', 'very well blended', 'well blended with', 'blended with the', 'with the studio', 'the studio &', 'studio & US', '& US locations', 'US locations making', 'locations making it', 'making it sometimes', 'it sometimes hard', 'sometimes hard to', 'hard to tell', 'to tell which', 'tell which is', 'which is which', 'is which .', 'which . Do', \". Do n't\", \"Do n't complain\", \"n't complain too\", 'complain too much', 'too much though', 'much though ,', 'though , remember', ', remember that', 'remember that 90', 'that 90 %', '90 % of', '% of ALL', 'of ALL films', 'ALL films is', 'films is phony', 'is phony anyway', 'phony anyway ,', 'anyway , so', ', so just', 'so just relax', 'just relax and', 'relax and enjoy', 'and enjoy the', 'enjoy the damned', 'the damned thing', 'damned thing with', 'thing with a', 'with a big', 'a big bowl', 'big bowl of', 'bowl of popcorn', 'of popcorn ,', 'popcorn , some', ', some cold', 'some cold beer', 'cold beer ,', 'beer , and', ', and a', 'and a fresh', 'a fresh pack', 'fresh pack of', 'pack of smokes', 'of smokes ...', 'smokes ... a', '... a sexy', 'a sexy and', 'sexy and willing', 'and willing girlfriend', 'willing girlfriend /', 'girlfriend / wife', '/ wife is', \"wife is n't\", \"is n't out\", \"n't out of\", 'out of line', 'of line either', 'line either ...', 'either ... lol', '... lol .', 'lol . Oh', '. Oh ...', 'Oh ... One', '... One final', 'One final word', 'final word about', 'word about nudity', 'about nudity ...', 'nudity ... at', '... at the', 'at the very', 'the very beginning', 'very beginning ,', 'beginning , while', ', while the', 'while the white', 'the white hunters', 'white hunters are', 'hunters are speaking', 'are speaking dialogue', 'speaking dialogue ,', 'dialogue , keep', ', keep your', 'keep your eyes', 'your eyes on', 'eyes on the', 'on the background', 'the background extras', 'background extras ...', 'extras ... there', '... there are', 'there are several', 'are several good', 'several good shots', 'good shots of', 'shots of nude', 'of nude African', 'nude African girls', 'African girls (', 'girls ( obviously', '( obviously shot', 'obviously shot on', 'shot on location', 'on location )', 'location ) behind', ') behind them', 'behind them .', 'them . One', '. One more', 'One more thing', 'more thing ,', 'thing , the', ', the movie', 'the movie is', 'movie is not', 'is not racist', 'not racist by', 'racist by the', 'by the standards', 'the standards of', 'standards of the', 'of the 1930', \"the 1930 's\", \"1930 's until\", \"'s until the\", \"until the 1960's\", \"the 1960's ...\", \"1960's ... that\", \"... that 's\", \"that 's the\", \"'s the way\", 'the way colored', 'way colored people', 'colored people were', 'people were thought', 'were thought of', 'thought of and', 'of and portrayed', 'and portrayed back', 'portrayed back then', 'back then .', 'then . Shaft', '. Shaft had', \"Shaft had n't\", \"had n't even\", \"n't even been\", 'even been thought', 'been thought about', 'thought about at', 'about at that', 'at that time', 'that time ,', 'time , nor', ', nor would', 'nor would audiences', 'would audiences have', 'audiences have accepted', 'have accepted any', 'accepted any other', 'any other portrayals', 'other portrayals of', 'portrayals of them', 'of them at', 'them at the', 'at the time', 'the time in', 'time in history', 'in history .', 'history . Safaris', '. Safaris actually', 'Safaris actually did', 'actually did use', 'did use natives', 'use natives carrying', 'natives carrying luggage', 'carrying luggage on', 'luggage on their', 'on their heads', 'their heads ...', 'heads ... and', '... and Tiny', \"and Tiny 's\", \"Tiny 's character\", \"'s character did\", 'character did die', 'did die a', 'die a heroic', 'a heroic death', 'heroic death trying', 'death trying to', 'trying to save', 'to save the', 'save the white', 'the white hunters', 'white hunters and', 'hunters and Jane', 'and Jane .', 'Jane . As', '. As a', 'As a matter', 'a matter of', 'matter of fact', 'of fact ,', 'fact , it', ', it was', \"it was n't\", \"was n't until\", \"n't until Gene\", 'until Gene Autry', 'Gene Autry treated', 'Autry treated the', 'treated the native', 'the native Americans', 'native Americans and', 'Americans and colored', 'and colored people', 'colored people in', 'people in his', 'in his Westerns', 'his Westerns like', 'Westerns like real', 'like real human', 'real human beings', 'human beings that', 'beings that Hollywood', 'that Hollywood began', 'Hollywood began to', 'began to see', 'to see that', 'see that it', 'that it was', 'it was okay', 'was okay to', 'okay to do', 'to do so', 'do so .'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHO-LV9xpCcG",
        "outputId": "6067b25e-f0ad-4117-c709-b249a9ad4b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Output of a dictionary of which shows the count of each trigram.\n",
        "dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Now I recently': 1,\n",
              " 'I recently had': 4,\n",
              " 'recently had the': 4,\n",
              " 'had the viewing': 1,\n",
              " 'the viewing pleasure': 2,\n",
              " 'viewing pleasure to': 2,\n",
              " 'pleasure to watch': 27,\n",
              " 'to watch the': 189,\n",
              " 'watch the hilarious': 1,\n",
              " 'the hilarious comedy': 1,\n",
              " 'hilarious comedy Bachelor': 1,\n",
              " 'comedy Bachelor Party': 1,\n",
              " 'Bachelor Party ,': 2,\n",
              " 'Party , one': 1,\n",
              " ', one of': 391,\n",
              " 'one of my': 451,\n",
              " 'of my new': 5,\n",
              " 'my new favorite': 2,\n",
              " 'new favorite comedies': 1,\n",
              " 'favorite comedies ,': 1,\n",
              " 'comedies , laughed': 1,\n",
              " ', laughed until': 1,\n",
              " 'laughed until it': 1,\n",
              " 'until it just': 1,\n",
              " 'it just hurt': 1,\n",
              " 'just hurt type': 1,\n",
              " 'hurt type of': 1,\n",
              " 'type of movies': 17,\n",
              " 'of movies .': 36,\n",
              " 'movies . So': 12,\n",
              " '. So I': 136,\n",
              " 'So I naturally': 1,\n",
              " 'I naturally wanted': 1,\n",
              " 'naturally wanted to': 1,\n",
              " 'wanted to see': 163,\n",
              " 'to see the': 506,\n",
              " 'see the sequel': 5,\n",
              " 'the sequel ,': 19,\n",
              " 'sequel , hoping': 1,\n",
              " ', hoping it': 8,\n",
              " 'hoping it would': 21,\n",
              " 'it would have': 253,\n",
              " 'would have the': 15,\n",
              " 'have the same': 60,\n",
              " 'the same laughs': 3,\n",
              " 'same laughs ,': 1,\n",
              " 'laughs , but': 18,\n",
              " ', but instead': 89,\n",
              " 'but instead Bachelor': 1,\n",
              " 'instead Bachelor Party': 1,\n",
              " 'Bachelor Party 2': 1,\n",
              " 'Party 2 :': 2,\n",
              " '2 : The': 9,\n",
              " ': The Last': 4,\n",
              " 'The Last Temptation': 4,\n",
              " 'Last Temptation is': 1,\n",
              " 'Temptation is made': 1,\n",
              " 'is made by': 11,\n",
              " 'made by the': 44,\n",
              " 'by the American': 3,\n",
              " 'the American Pie': 8,\n",
              " 'American Pie generation': 1,\n",
              " 'Pie generation where': 1,\n",
              " 'generation where it': 1,\n",
              " \"where it 's\": 41,\n",
              " \"it 's tasteless\": 1,\n",
              " \"'s tasteless and\": 1,\n",
              " 'tasteless and defeats': 1,\n",
              " 'and defeats the': 1,\n",
              " 'defeats the hole': 1,\n",
              " 'the hole purpose': 1,\n",
              " 'hole purpose of': 1,\n",
              " 'purpose of the': 21,\n",
              " 'of the first': 318,\n",
              " 'the first film': 179,\n",
              " 'first film .': 38,\n",
              " 'film . Yeah': 2,\n",
              " '. Yeah ,': 64,\n",
              " 'Yeah , the': 4,\n",
              " ', the first': 117,\n",
              " 'first film has': 4,\n",
              " 'film has nudity': 1,\n",
              " 'has nudity ,': 2,\n",
              " 'nudity , but': 17,\n",
              " ', but it': 2057,\n",
              " 'but it does': 164,\n",
              " \"it does n't\": 548,\n",
              " \"does n't show\": 29,\n",
              " \"n't show in\": 1,\n",
              " 'show in every': 1,\n",
              " 'in every single': 10,\n",
              " 'every single scene': 8,\n",
              " 'single scene .': 2,\n",
              " 'scene . Also': 3,\n",
              " '. Also the': 85,\n",
              " 'Also the plot': 2,\n",
              " 'the plot is': 262,\n",
              " 'plot is exactly': 1,\n",
              " 'is exactly the': 22,\n",
              " 'exactly the same': 69,\n",
              " 'the same from': 4,\n",
              " 'same from the': 4,\n",
              " 'from the first': 158,\n",
              " 'the first ,': 50,\n",
              " 'first , it': 14,\n",
              " \", it 's\": 2403,\n",
              " \"it 's not\": 799,\n",
              " \"'s not always\": 8,\n",
              " 'not always a': 4,\n",
              " 'always a complaint': 1,\n",
              " 'a complaint with': 1,\n",
              " 'complaint with me': 1,\n",
              " 'with me ,': 24,\n",
              " 'me , but': 79,\n",
              " ', but this': 842,\n",
              " 'but this could': 5,\n",
              " 'this could have': 53,\n",
              " 'could have been': 804,\n",
              " 'have been a': 487,\n",
              " 'been a little': 28,\n",
              " 'a little more': 195,\n",
              " 'little more original': 3,\n",
              " 'more original .': 2,\n",
              " 'original . The': 39,\n",
              " '. The only': 773,\n",
              " 'The only thing': 183,\n",
              " 'only thing is': 6,\n",
              " 'thing is that': 74,\n",
              " 'is that I': 80,\n",
              " \"that I 'm\": 112,\n",
              " \"I 'm glad\": 121,\n",
              " \"'m glad that\": 24,\n",
              " 'glad that at': 2,\n",
              " 'that at least': 37,\n",
              " 'at least no': 4,\n",
              " 'least no old': 1,\n",
              " 'no old actors': 1,\n",
              " 'old actors from': 1,\n",
              " 'actors from the': 11,\n",
              " 'from the original': 97,\n",
              " 'the original appear': 1,\n",
              " 'original appear in': 1,\n",
              " 'appear in this': 18,\n",
              " 'in this movie': 1260,\n",
              " 'this movie ,': 1069,\n",
              " 'movie , because': 43,\n",
              " ', because it': 209,\n",
              " 'because it would': 11,\n",
              " 'would have been': 762,\n",
              " 'have been cheesy': 1,\n",
              " 'been cheesy or': 1,\n",
              " 'cheesy or really': 1,\n",
              " 'or really silly': 1,\n",
              " 'really silly looking.<br': 1,\n",
              " 'silly looking.<br /><br': 1,\n",
              " 'looking.<br /><br />Ron': 1,\n",
              " '/><br />Ron and': 1,\n",
              " '/>Ron and Melinda': 1,\n",
              " 'and Melinda are': 1,\n",
              " 'Melinda are engaged': 1,\n",
              " 'are engaged ,': 1,\n",
              " 'engaged , after': 1,\n",
              " ', after only': 2,\n",
              " 'after only 2': 3,\n",
              " 'only 2 months': 1,\n",
              " '2 months of': 3,\n",
              " 'months of dating': 1,\n",
              " 'of dating ,': 1,\n",
              " 'dating , everyone': 1,\n",
              " ', everyone is': 10,\n",
              " 'everyone is against': 2,\n",
              " 'is against it': 2,\n",
              " 'against it .': 19,\n",
              " 'it . Melinda': 1,\n",
              " '. Melinda has': 1,\n",
              " 'Melinda has a': 1,\n",
              " 'has a rich': 3,\n",
              " 'a rich family': 2,\n",
              " 'rich family ,': 2,\n",
              " 'family , but': 23,\n",
              " ', but they': 475,\n",
              " \"but they 're\": 69,\n",
              " \"they 're pretty\": 6,\n",
              " \"'re pretty happy\": 1,\n",
              " 'pretty happy with': 2,\n",
              " 'happy with Ron': 1,\n",
              " 'with Ron ,': 1,\n",
              " 'Ron , and': 1,\n",
              " ', and Melinda': 1,\n",
              " \"and Melinda 's\": 1,\n",
              " \"Melinda 's brother\": 1,\n",
              " \"'s brother ,\": 17,\n",
              " 'brother , Todd': 1,\n",
              " ', Todd is': 1,\n",
              " 'Todd is scared': 1,\n",
              " 'is scared that': 1,\n",
              " 'scared that Ron': 1,\n",
              " 'that Ron will': 1,\n",
              " 'Ron will take': 1,\n",
              " 'will take his': 2,\n",
              " 'take his job': 1,\n",
              " 'his job .': 15,\n",
              " 'job . So': 3,\n",
              " '. So they': 18,\n",
              " 'So they go': 2,\n",
              " 'they go out': 9,\n",
              " 'go out on': 8,\n",
              " 'out on a': 56,\n",
              " 'on a weekend': 9,\n",
              " 'a weekend to': 1,\n",
              " 'weekend to Miami': 1,\n",
              " 'to Miami for': 1,\n",
              " 'Miami for a': 1,\n",
              " 'for a bachelor': 1,\n",
              " 'a bachelor party': 1,\n",
              " 'bachelor party and': 1,\n",
              " 'party and Todd': 1,\n",
              " 'and Todd is': 1,\n",
              " 'Todd is going': 1,\n",
              " 'is going to': 264,\n",
              " 'going to make': 40,\n",
              " 'to make sure': 69,\n",
              " 'make sure that': 25,\n",
              " 'sure that he': 4,\n",
              " \"that he 'll\": 10,\n",
              " \"he 'll trap\": 1,\n",
              " \"'ll trap Ron\": 1,\n",
              " 'trap Ron into': 1,\n",
              " 'Ron into a': 1,\n",
              " 'into a picture': 2,\n",
              " 'a picture that': 5,\n",
              " 'picture that will': 2,\n",
              " 'that will make': 40,\n",
              " 'will make Melinda': 1,\n",
              " 'make Melinda change': 1,\n",
              " 'Melinda change her': 1,\n",
              " 'change her mind': 1,\n",
              " 'her mind about': 1,\n",
              " 'mind about the': 2,\n",
              " 'about the marriage.<br': 1,\n",
              " 'the marriage.<br /><br': 2,\n",
              " 'marriage.<br /><br />Bachelor': 1,\n",
              " '/><br />Bachelor Party': 1,\n",
              " '/>Bachelor Party 2': 1,\n",
              " 'Last Temptation has': 1,\n",
              " 'Temptation has a': 1,\n",
              " 'has a couple': 24,\n",
              " 'a couple laughs': 1,\n",
              " 'couple laughs here': 1,\n",
              " 'laughs here and': 6,\n",
              " 'here and there': 123,\n",
              " 'and there ,': 40,\n",
              " 'there , but': 71,\n",
              " ', but over': 8,\n",
              " 'but over all': 7,\n",
              " 'over all fails': 1,\n",
              " 'all fails to': 1,\n",
              " 'fails to deliver': 24,\n",
              " 'to deliver what': 2,\n",
              " 'deliver what the': 1,\n",
              " 'what the first': 5,\n",
              " 'first film accomplished': 1,\n",
              " 'film accomplished .': 1,\n",
              " 'accomplished . These': 1,\n",
              " '. These guys': 19,\n",
              " 'These guys ,': 2,\n",
              " 'guys , Ron': 1,\n",
              " \", Ron 's\": 3,\n",
              " \"Ron 's friends\": 1,\n",
              " \"'s friends ,\": 11,\n",
              " 'friends , were': 1,\n",
              " ', were more': 2,\n",
              " 'were more obnoxious': 1,\n",
              " 'more obnoxious than': 1,\n",
              " 'obnoxious than likable': 1,\n",
              " 'than likable ,': 2,\n",
              " 'likable , except': 1,\n",
              " ', except for': 103,\n",
              " 'except for Seth': 1,\n",
              " 'for Seth ,': 1,\n",
              " 'Seth , he': 1,\n",
              " ', he was': 176,\n",
              " 'he was kinda': 1,\n",
              " 'was kinda funny': 2,\n",
              " 'kinda funny .': 4,\n",
              " 'funny . The': 75,\n",
              " 'The only likable': 2,\n",
              " 'only likable characters': 1,\n",
              " 'likable characters other': 1,\n",
              " 'characters other than': 4,\n",
              " 'other than Seth': 1,\n",
              " 'than Seth is': 1,\n",
              " 'Seth is Ron': 1,\n",
              " 'is Ron and': 1,\n",
              " 'Ron and Melinda': 1,\n",
              " 'and Melinda ,': 3,\n",
              " 'Melinda , everyone': 1,\n",
              " ', everyone else': 9,\n",
              " 'everyone else just': 2,\n",
              " 'else just more': 1,\n",
              " 'just more or': 1,\n",
              " 'more or less': 113,\n",
              " 'or less gets': 1,\n",
              " 'less gets on': 1,\n",
              " 'gets on your': 1,\n",
              " 'on your nerves': 5,\n",
              " 'your nerves .': 2,\n",
              " 'nerves . You': 1,\n",
              " '. You wanna': 3,\n",
              " 'You wanna watch': 1,\n",
              " 'wanna watch this': 3,\n",
              " 'watch this film': 145,\n",
              " 'this film ?': 64,\n",
              " 'film ? Just': 1,\n",
              " '? Just watch': 4,\n",
              " 'Just watch Girls': 1,\n",
              " 'watch Girls Gone': 1,\n",
              " 'Girls Gone Wild': 1,\n",
              " 'Gone Wild ,': 1,\n",
              " 'Wild , it': 1,\n",
              " \"it 's the\": 336,\n",
              " \"'s the same\": 41,\n",
              " 'the same thing': 84,\n",
              " 'same thing only': 1,\n",
              " 'thing only it': 1,\n",
              " 'only it does': 2,\n",
              " \"does n't try\": 34,\n",
              " \"n't try to\": 67,\n",
              " 'try to pretend': 4,\n",
              " 'to pretend that': 9,\n",
              " 'pretend that it': 4,\n",
              " \"that it 's\": 578,\n",
              " \"it 's a\": 1133,\n",
              " \"'s a film\": 39,\n",
              " 'a film .': 143,\n",
              " 'film . Stick': 1,\n",
              " '. Stick to': 9,\n",
              " 'Stick to the': 6,\n",
              " 'to the original': 142,\n",
              " 'the original Bachelor': 1,\n",
              " 'original Bachelor Party': 1,\n",
              " 'Party , that': 1,\n",
              " \", that 's\": 355,\n",
              " \"that 's the\": 188,\n",
              " \"'s the movie\": 13,\n",
              " 'the movie that': 137,\n",
              " \"movie that 's\": 40,\n",
              " \"that 's going\": 14,\n",
              " \"'s going to\": 112,\n",
              " 'going to get': 81,\n",
              " 'to get you': 21,\n",
              " 'get you in': 3,\n",
              " 'you in tears': 3,\n",
              " 'in tears of': 2,\n",
              " 'tears of laughter.<br': 1,\n",
              " 'of laughter.<br /><br': 2,\n",
              " 'laughter.<br /><br />3/10': 1,\n",
              " 'I have given': 17,\n",
              " 'have given this': 34,\n",
              " 'given this film': 15,\n",
              " 'this film an': 8,\n",
              " 'film an elevated': 1,\n",
              " 'an elevated rating': 1,\n",
              " 'elevated rating of': 1,\n",
              " 'rating of 2': 3,\n",
              " 'of 2 stars': 2,\n",
              " '2 stars as': 1,\n",
              " 'stars as I': 1,\n",
              " 'as I personally': 2,\n",
              " 'I personally appear': 1,\n",
              " 'personally appear in': 1,\n",
              " 'appear in minutes': 1,\n",
              " 'in minutes 42': 1,\n",
              " 'minutes 42 and': 1,\n",
              " '42 and 43': 1,\n",
              " 'and 43 of': 1,\n",
              " '43 of the': 1,\n",
              " 'of the film': 2232,\n",
              " 'the film ....': 3,\n",
              " 'film .... the': 2,\n",
              " '.... the road': 1,\n",
              " 'the road side': 2,\n",
              " 'road side bar': 1,\n",
              " 'side bar scene': 1,\n",
              " 'bar scene in': 1,\n",
              " 'scene in Russia': 1,\n",
              " 'in Russia .': 2,\n",
              " 'Russia . In': 1,\n",
              " '. In this': 294,\n",
              " 'In this scene': 6,\n",
              " 'this scene the': 2,\n",
              " 'scene the director': 2,\n",
              " 'the director of': 72,\n",
              " 'director of the': 34,\n",
              " 'of the movie': 1884,\n",
              " 'the movie offered': 1,\n",
              " 'movie offered me': 1,\n",
              " 'offered me the': 2,\n",
              " 'me the immortal': 1,\n",
              " 'the immortal line': 2,\n",
              " 'immortal line -': 1,\n",
              " 'line - \"': 2,\n",
              " '- \" 50': 1,\n",
              " '\" 50 Dollars': 1,\n",
              " '50 Dollars ..': 1,\n",
              " 'Dollars .. you': 1,\n",
              " '.. you Drink': 1,\n",
              " 'you Drink and': 1,\n",
              " 'Drink and Talk': 1,\n",
              " 'and Talk \"': 1,\n",
              " 'Talk \" ,': 1,\n",
              " '\" , but': 209,\n",
              " ', but I': 1740,\n",
              " 'but I felt': 36,\n",
              " 'I felt that': 67,\n",
              " 'felt that my': 1,\n",
              " 'that my Polish': 1,\n",
              " 'my Polish counterpart': 1,\n",
              " 'Polish counterpart could': 1,\n",
              " 'counterpart could speak': 1,\n",
              " 'could speak in': 1,\n",
              " 'speak in a': 9,\n",
              " 'in a more': 36,\n",
              " 'a more convincing': 3,\n",
              " 'more convincing Russian': 1,\n",
              " 'convincing Russian accent': 1,\n",
              " 'Russian accent than': 1,\n",
              " 'accent than I': 1,\n",
              " 'than I could': 11,\n",
              " 'I could ,': 7,\n",
              " 'could , so': 1,\n",
              " ', so I': 398,\n",
              " 'so I declined': 1,\n",
              " 'I declined to': 1,\n",
              " 'declined to take': 1,\n",
              " 'to take this': 25,\n",
              " 'take this speaking': 1,\n",
              " 'this speaking part': 1,\n",
              " 'speaking part on': 1,\n",
              " 'part on .': 1,\n",
              " 'on . I': 61,\n",
              " '. I was': 1010,\n",
              " 'I was slightly': 6,\n",
              " 'was slightly starstruck': 1,\n",
              " 'slightly starstruck as': 1,\n",
              " 'starstruck as this': 1,\n",
              " 'as this was': 10,\n",
              " 'this was my': 7,\n",
              " 'was my first': 18,\n",
              " 'my first Film': 1,\n",
              " 'first Film experience': 1,\n",
              " 'Film experience ....': 1,\n",
              " 'experience .... and': 1,\n",
              " '.... and who': 1,\n",
              " 'and who knows': 2,\n",
              " 'who knows ...': 3,\n",
              " 'knows ... these': 1,\n",
              " '... these lines': 1,\n",
              " 'these lines could': 1,\n",
              " 'lines could have': 2,\n",
              " 'could have ended': 6,\n",
              " 'have ended up': 7,\n",
              " 'ended up there': 1,\n",
              " 'up there with': 65,\n",
              " 'there with lines': 1,\n",
              " 'with lines such': 1,\n",
              " 'lines such as': 10,\n",
              " 'such as \"': 109,\n",
              " 'as \" I': 8,\n",
              " '\" I \\'ll': 45,\n",
              " \"I 'll be\": 84,\n",
              " \"'ll be Back\": 1,\n",
              " 'be Back \"': 1,\n",
              " 'Back \" and': 2,\n",
              " '\" and \"': 649,\n",
              " 'and \" Quite': 1,\n",
              " '\" Quite Frankly': 1,\n",
              " 'Quite Frankly My': 1,\n",
              " 'Frankly My Dear': 1,\n",
              " 'My Dear ,': 1,\n",
              " 'Dear , I': 1,\n",
              " ', I Do': 1,\n",
              " \"I Do n't\": 2,\n",
              " \"Do n't Give\": 1,\n",
              " \"n't Give a\": 1,\n",
              " 'Give a Damn': 1,\n",
              " 'a Damn \"': 1,\n",
              " 'Damn \" .': 1,\n",
              " '\" . Had': 2,\n",
              " '. Had I': 10,\n",
              " 'Had I spoken': 1,\n",
              " 'I spoken that': 1,\n",
              " 'spoken that one': 1,\n",
              " 'that one line': 2,\n",
              " 'one line then': 1,\n",
              " 'line then my': 1,\n",
              " 'then my name': 1,\n",
              " 'my name would': 1,\n",
              " 'name would appear': 1,\n",
              " 'would appear in': 8,\n",
              " 'appear in the': 25,\n",
              " 'in the credits': 43,\n",
              " 'the credits of': 5,\n",
              " 'credits of Rancid': 1,\n",
              " 'of Rancid Aluminium': 2,\n",
              " 'Rancid Aluminium as': 1,\n",
              " \"Aluminium as '\": 1,\n",
              " \"as ' Heavy\": 1,\n",
              " \"' Heavy 1\": 1,\n",
              " \"Heavy 1 '\": 1,\n",
              " \"1 ' instead\": 1,\n",
              " \"' instead of\": 1,\n",
              " 'instead of the': 73,\n",
              " 'of the name': 6,\n",
              " 'the name of': 157,\n",
              " 'name of Ryszard': 1,\n",
              " 'of Ryszard Janikowski': 1,\n",
              " 'Ryszard Janikowski .': 1,\n",
              " 'Janikowski . <': 1,\n",
              " '. < br': 3565,\n",
              " '< br /><br': 6534,\n",
              " 'br /><br />As': 76,\n",
              " '/><br />As time': 1,\n",
              " '/>As time goes': 1,\n",
              " 'time goes on': 1,\n",
              " 'goes on ,': 22,\n",
              " 'on , I': 35,\n",
              " ', I am': 238,\n",
              " 'I am counting': 1,\n",
              " 'am counting myself': 1,\n",
              " 'counting myself lucky': 1,\n",
              " 'myself lucky that': 1,\n",
              " 'lucky that my': 1,\n",
              " 'that my name': 1,\n",
              " 'my name is': 3,\n",
              " 'name is in': 1,\n",
              " 'is in no': 19,\n",
              " 'in no way': 68,\n",
              " 'no way connected': 2,\n",
              " 'way connected to': 3,\n",
              " 'connected to this': 4,\n",
              " 'to this film.<br': 6,\n",
              " 'this film.<br /><br': 156,\n",
              " 'film.<br /><br />Even': 1,\n",
              " '/><br />Even though': 41,\n",
              " '/>Even though I': 4,\n",
              " 'though I spent': 2,\n",
              " 'I spent a': 3,\n",
              " 'spent a whole': 1,\n",
              " 'a whole day': 3,\n",
              " 'whole day on': 1,\n",
              " 'day on the': 6,\n",
              " 'on the set': 50,\n",
              " 'the set ,': 19,\n",
              " 'set , in': 2,\n",
              " ', in South': 3,\n",
              " 'in South Wales': 1,\n",
              " 'South Wales hot': 1,\n",
              " 'Wales hot -': 1,\n",
              " 'hot - spot': 1,\n",
              " '- spot Barry': 1,\n",
              " 'spot Barry Island': 1,\n",
              " 'Barry Island ,': 1,\n",
              " 'Island , no': 1,\n",
              " ', no one': 83,\n",
              " 'no one could': 24,\n",
              " 'one could tell': 2,\n",
              " 'could tell me': 2,\n",
              " 'tell me what': 16,\n",
              " 'me what the': 7,\n",
              " 'what the actual': 2,\n",
              " 'the actual storyline': 2,\n",
              " 'actual storyline was': 1,\n",
              " 'storyline was .': 1,\n",
              " 'was . The': 34,\n",
              " '. The caterers': 1,\n",
              " 'The caterers and': 1,\n",
              " 'caterers and the': 1,\n",
              " 'and the wardrobe': 2,\n",
              " 'the wardrobe lady': 2,\n",
              " 'wardrobe lady all': 1,\n",
              " 'lady all concurred': 1,\n",
              " 'all concurred that': 1,\n",
              " 'concurred that it': 1,\n",
              " 'that it appeared': 3,\n",
              " 'it appeared to': 5,\n",
              " 'appeared to have': 8,\n",
              " 'to have a': 363,\n",
              " 'have a lot': 87,\n",
              " 'a lot of': 1906,\n",
              " 'lot of swearing': 2,\n",
              " 'of swearing and': 1,\n",
              " 'swearing and nudity': 3,\n",
              " 'and nudity in': 1,\n",
              " 'nudity in it': 2,\n",
              " 'in it .....': 1,\n",
              " 'it ..... things': 1,\n",
              " '..... things could': 1,\n",
              " 'things could certainly': 1,\n",
              " 'could certainly have': 2,\n",
              " 'certainly have been': 2,\n",
              " 'have been worse': 9,\n",
              " 'been worse if': 1,\n",
              " 'worse if I': 1,\n",
              " \"if I 'd\": 18,\n",
              " \"I 'd ended\": 1,\n",
              " \"'d ended up\": 1,\n",
              " 'ended up naked': 1,\n",
              " 'up naked in': 1,\n",
              " 'naked in this': 2,\n",
              " 'in this most': 3,\n",
              " 'this most dreadful': 1,\n",
              " 'most dreadful of': 2,\n",
              " 'dreadful of films': 1,\n",
              " 'of films ....': 1,\n",
              " 'films .... <br': 1,\n",
              " '.... <br /><br': 105,\n",
              " '<br /><br />Still': 2,\n",
              " '/><br />Still .....': 1,\n",
              " '/>Still ..... On': 1,\n",
              " '..... On the': 1,\n",
              " 'On the positive': 7,\n",
              " 'the positive side': 20,\n",
              " 'positive side ....': 1,\n",
              " 'side .... I': 1,\n",
              " '.... I got': 2,\n",
              " 'I got chatting': 1,\n",
              " 'got chatting to': 1,\n",
              " 'chatting to Rhys': 1,\n",
              " 'to Rhys Ifans': 1,\n",
              " 'Rhys Ifans during': 1,\n",
              " 'Ifans during one': 1,\n",
              " 'during one break': 1,\n",
              " 'one break .': 1,\n",
              " 'break . I': 5,\n",
              " '. I had': 303,\n",
              " 'I had no': 91,\n",
              " 'had no idea': 83,\n",
              " 'no idea who': 15,\n",
              " 'idea who he': 1,\n",
              " 'who he was': 20,\n",
              " 'he was ,': 21,\n",
              " 'was , as': 18,\n",
              " ', as \"': 19,\n",
              " 'as \" Notting': 1,\n",
              " '\" Notting Hill': 2,\n",
              " 'Notting Hill \"': 1,\n",
              " 'Hill \" was': 3,\n",
              " '\" was yet': 1,\n",
              " 'was yet to': 4,\n",
              " 'yet to be': 18,\n",
              " 'to be released': 34,\n",
              " 'be released ,': 4,\n",
              " 'released , and': 15,\n",
              " ', and not': 229,\n",
              " 'and not an': 4,\n",
              " 'not an inkling': 1,\n",
              " 'an inkling that': 2,\n",
              " 'inkling that he': 1,\n",
              " 'that he might': 16,\n",
              " 'he might be': 17,\n",
              " 'might be Welsh': 1,\n",
              " 'be Welsh .': 1,\n",
              " 'Welsh . Made': 1,\n",
              " '. Made various': 1,\n",
              " 'Made various inappropriate': 1,\n",
              " 'various inappropriate comments': 1,\n",
              " 'inappropriate comments about': 1,\n",
              " 'comments about what': 2,\n",
              " 'about what an': 2,\n",
              " 'what an awful': 4,\n",
              " 'an awful pit': 1,\n",
              " 'awful pit Barry': 1,\n",
              " 'pit Barry Island': 1,\n",
              " 'Barry Island had': 1,\n",
              " 'Island had become': 1,\n",
              " 'had become since': 1,\n",
              " 'become since my': 1,\n",
              " 'since my childhood': 2,\n",
              " 'my childhood visits': 1,\n",
              " 'childhood visits there': 1,\n",
              " 'visits there in': 1,\n",
              " 'there in the': 42,\n",
              " 'in the 70s': 30,\n",
              " 'the 70s and': 10,\n",
              " '70s and 80s': 7,\n",
              " 'and 80s .': 1,\n",
              " '80s . It': 2,\n",
              " '. It was': 1585,\n",
              " 'It was only': 14,\n",
              " 'was only when': 3,\n",
              " 'only when Keith': 1,\n",
              " 'when Keith Allen': 1,\n",
              " 'Keith Allen showed': 1,\n",
              " 'Allen showed up': 1,\n",
              " 'showed up that': 1,\n",
              " 'up that I': 1,\n",
              " 'that I realised': 2,\n",
              " 'I realised I': 3,\n",
              " 'realised I was': 2,\n",
              " 'I was in': 148,\n",
              " 'was in a': 36,\n",
              " 'in a quality': 2,\n",
              " 'a quality production': 3,\n",
              " 'quality production ........': 1,\n",
              " 'This could be': 26,\n",
              " 'could be the': 34,\n",
              " 'be the most': 68,\n",
              " 'the most underrated': 23,\n",
              " 'most underrated movie': 1,\n",
              " 'underrated movie of': 1,\n",
              " 'movie of its': 11,\n",
              " 'of its genre': 13,\n",
              " 'its genre .': 10,\n",
              " 'genre . I': 22,\n",
              " '. I do': 858,\n",
              " \"I do n't\": 2092,\n",
              " \"do n't remember\": 78,\n",
              " \"n't remember seeing\": 5,\n",
              " 'remember seeing any': 2,\n",
              " 'seeing any advertisements': 1,\n",
              " 'any advertisements or': 1,\n",
              " 'advertisements or commercials': 1,\n",
              " 'or commercials for': 1,\n",
              " 'commercials for this': 4,\n",
              " 'for this one': 63,\n",
              " 'this one which': 4,\n",
              " 'one which could': 1,\n",
              " 'which could be': 10,\n",
              " 'be the reason': 13,\n",
              " 'the reason why': 50,\n",
              " 'reason why it': 10,\n",
              " 'why it did': 14,\n",
              " \"it did n't\": 252,\n",
              " \"did n't do\": 68,\n",
              " \"n't do so\": 5,\n",
              " 'do so well': 7,\n",
              " 'so well at': 6,\n",
              " 'well at the': 15,\n",
              " 'at the box': 64,\n",
              " 'the box office': 71,\n",
              " 'box office .': 23,\n",
              " 'office . However': 1,\n",
              " '. However ,': 1277,\n",
              " 'However , Frailty': 1,\n",
              " ', Frailty is': 1,\n",
              " 'Frailty is an': 1,\n",
              " 'is an excellent': 118,\n",
              " 'an excellent and': 11,\n",
              " 'excellent and a': 3,\n",
              " 'and a truly': 10,\n",
              " 'a truly original': 6,\n",
              " 'truly original horror': 1,\n",
              " 'original horror movie': 2,\n",
              " 'horror movie .': 68,\n",
              " 'movie . I': 571,\n",
              " '. I rank': 6,\n",
              " 'I rank it': 5,\n",
              " 'rank it within': 1,\n",
              " 'it within the': 2,\n",
              " 'within the top': 2,\n",
              " 'the top 10': 6,\n",
              " 'top 10 most': 2,\n",
              " '10 most favorite': 1,\n",
              " 'most favorite horror': 2,\n",
              " 'favorite horror movies': 1,\n",
              " 'horror movies on': 2,\n",
              " 'movies on my': 1,\n",
              " 'on my list.<br': 1,\n",
              " 'my list.<br /><br': 3,\n",
              " 'list.<br /><br />Movie': 1,\n",
              " '/><br />Movie begins': 1,\n",
              " '/>Movie begins with': 1,\n",
              " 'begins with snapshots': 1,\n",
              " 'with snapshots of': 1,\n",
              " 'snapshots of photos': 1,\n",
              " 'of photos and': 1,\n",
              " 'photos and news': 1,\n",
              " 'and news articles': 2,\n",
              " 'news articles telling': 1,\n",
              " 'articles telling us': 1,\n",
              " 'telling us about': 2,\n",
              " 'us about a': 3,\n",
              " 'about a killer': 12,\n",
              " 'a killer who': 7,\n",
              " 'killer who calls': 1,\n",
              " 'who calls himself': 6,\n",
              " 'calls himself \"': 3,\n",
              " 'himself \" God': 1,\n",
              " '\" God \\'s': 3,\n",
              " \"God 's hand\": 2,\n",
              " '\\'s hand \"': 1,\n",
              " 'hand \" .': 1,\n",
              " '\" . And': 43,\n",
              " '. And then': 109,\n",
              " 'And then a': 2,\n",
              " 'then a man': 2,\n",
              " 'a man walks': 1,\n",
              " 'man walks into': 1,\n",
              " 'walks into a': 6,\n",
              " 'into a police': 2,\n",
              " 'a police station': 6,\n",
              " 'police station and': 2,\n",
              " 'station and tells': 1,\n",
              " 'and tells the': 15,\n",
              " 'tells the chief': 1,\n",
              " 'the chief officer': 1,\n",
              " 'chief officer that': 1,\n",
              " 'officer that he': 1,\n",
              " 'that he knows': 17,\n",
              " 'he knows the': 6,\n",
              " 'knows the killer': 2,\n",
              " 'the killer is': 40,\n",
              " 'killer is his': 2,\n",
              " 'is his brother': 2,\n",
              " 'his brother .': 14,\n",
              " 'brother . Two': 1,\n",
              " '. Two of': 20,\n",
              " 'Two of them': 6,\n",
              " 'of them leave': 2,\n",
              " 'them leave together': 1,\n",
              " 'leave together to': 1,\n",
              " 'together to go': 1,\n",
              " 'to go to': 202,\n",
              " 'go to a': 52,\n",
              " 'to a location': 1,\n",
              " 'a location where': 1,\n",
              " 'location where victims': 1,\n",
              " 'where victims are': 1,\n",
              " 'victims are buried': 1,\n",
              " 'are buried which': 1,\n",
              " 'buried which might': 1,\n",
              " 'which might help': 1,\n",
              " 'might help solve': 1,\n",
              " 'help solve the': 2,\n",
              " 'solve the case': 9,\n",
              " 'the case .': 53,\n",
              " 'case . During': 1,\n",
              " '. During that': 1,\n",
              " 'During that trip': 1,\n",
              " 'that trip ,': 2,\n",
              " 'trip , the': 4,\n",
              " ', the man': 62,\n",
              " 'the man begins': 1,\n",
              " 'man begins telling': 1,\n",
              " 'begins telling the': 1,\n",
              " 'telling the story': 32,\n",
              " 'the story of': 410,\n",
              " 'story of his': 19,\n",
              " 'of his brother': 13,\n",
              " 'his brother and': 18,\n",
              " 'brother and we': 1,\n",
              " 'and we go': 2,\n",
              " 'we go back': 5,\n",
              " 'go back in': 14,\n",
              " 'back in time': 52,\n",
              " 'in time when': 1,\n",
              " 'time when the': 16,\n",
              " 'when the events': 2,\n",
              " 'the events began': 1,\n",
              " 'events began .': 1,\n",
              " 'began . Fenton': 1,\n",
              " '. Fenton and': 1,\n",
              " 'Fenton and Adam': 1,\n",
              " 'and Adam are': 1,\n",
              " 'Adam are two': 1,\n",
              " 'are two young': 4,\n",
              " 'two young brothers': 2,\n",
              " 'young brothers living': 1,\n",
              " 'brothers living with': 1,\n",
              " 'living with their': 1,\n",
              " 'with their strict': 1,\n",
              " 'their strict and': 1,\n",
              " 'strict and religious': 1,\n",
              " 'and religious father': 1,\n",
              " 'religious father who': 1,\n",
              " 'father who ,': 2,\n",
              " 'who , one': 2,\n",
              " ', one day': 10,\n",
              " 'one day ,': 34,\n",
              " 'day , claims': 1,\n",
              " ', claims that': 1,\n",
              " 'claims that he': 4,\n",
              " 'that he has': 141,\n",
              " 'he has received': 3,\n",
              " 'has received a': 6,\n",
              " 'received a divine': 1,\n",
              " 'a divine message': 1,\n",
              " 'divine message from': 1,\n",
              " 'message from God': 1,\n",
              " 'from God asking': 1,\n",
              " 'God asking him': 1,\n",
              " 'asking him to': 5,\n",
              " 'him to kill': 9,\n",
              " 'to kill the': 75,\n",
              " 'kill the demons': 1,\n",
              " 'the demons that': 2,\n",
              " 'demons that appear': 1,\n",
              " 'that appear to': 8,\n",
              " 'appear to be': 75,\n",
              " 'to be regular': 1,\n",
              " 'be regular human': 1,\n",
              " 'regular human beings': 1,\n",
              " 'human beings .': 14,\n",
              " 'beings . He': 1,\n",
              " '. He receives': 5,\n",
              " 'He receives from': 1,\n",
              " 'receives from God': 1,\n",
              " 'from God a': 1,\n",
              " 'God a list': 1,\n",
              " 'a list of': 30,\n",
              " 'list of names': 2,\n",
              " 'of names of': 1,\n",
              " 'names of demons': 1,\n",
              " 'of demons to': 1,\n",
              " 'demons to be': 1,\n",
              " 'to be destroyed': 7,\n",
              " 'be destroyed and': 2,\n",
              " 'destroyed and asks': 1,\n",
              " 'and asks his': 1,\n",
              " 'asks his sons': 1,\n",
              " 'his sons to': 2,\n",
              " 'sons to help': 1,\n",
              " 'to help him': 62,\n",
              " 'help him carry': 1,\n",
              " 'him carry out': 1,\n",
              " 'carry out this': 1,\n",
              " 'out this divine': 1,\n",
              " 'this divine mission.<br': 1,\n",
              " 'divine mission.<br /><br': 1,\n",
              " 'mission.<br /><br />This': 1,\n",
              " '/><br />This is': 636,\n",
              " '/>This is an': 27,\n",
              " 'is an absolutely': 13,\n",
              " 'an absolutely horrifying': 1,\n",
              " 'absolutely horrifying and': 1,\n",
              " 'horrifying and suspenseful': 1,\n",
              " 'and suspenseful film': 1,\n",
              " 'suspenseful film that': 1,\n",
              " 'film that will': 36,\n",
              " 'that will keep': 17,\n",
              " 'will keep you': 39,\n",
              " 'keep you at': 1,\n",
              " 'you at the': 7,\n",
              " 'at the edge': 15,\n",
              " 'the edge of': 102,\n",
              " 'edge of your': 35,\n",
              " 'of your seat': 33,\n",
              " 'your seat .': 12,\n",
              " 'seat . The': 9,\n",
              " '. The tension': 15,\n",
              " 'The tension runs': 1,\n",
              " 'tension runs high': 1,\n",
              " 'runs high ,': 1,\n",
              " 'high , innocent': 1,\n",
              " ', innocent people': 1,\n",
              " 'innocent people (': 1,\n",
              " 'people ( or': 2,\n",
              " '( or demons': 1,\n",
              " 'or demons ?': 1,\n",
              " 'demons ? )': 1,\n",
              " '? ) get': 1,\n",
              " ') get killed': 1,\n",
              " 'get killed and': 3,\n",
              " 'killed and religious': 1,\n",
              " 'and religious experiences': 1,\n",
              " 'religious experiences are': 1,\n",
              " 'experiences are questioned': 1,\n",
              " 'are questioned .': 1,\n",
              " 'questioned . It': 1,\n",
              " '. It has': 432,\n",
              " 'It has not': 1,\n",
              " 'has not one': 4,\n",
              " 'not one but': 13,\n",
              " 'one but few': 1,\n",
              " 'but few very': 1,\n",
              " 'few very intelligent': 1,\n",
              " 'very intelligent twists': 1,\n",
              " 'intelligent twists at': 1,\n",
              " 'twists at the': 7,\n",
              " 'at the end': 946,\n",
              " 'the end .': 458,\n",
              " 'end . If': 14,\n",
              " '. If you': 1723,\n",
              " 'If you like': 179,\n",
              " 'you like this': 23,\n",
              " 'like this genre': 4,\n",
              " 'this genre ,': 15,\n",
              " 'genre , I': 6,\n",
              " ', I highly': 27,\n",
              " 'I highly recommend': 131,\n",
              " 'highly recommend Frailty': 1,\n",
              " 'recommend Frailty for': 1,\n",
              " 'Frailty for you': 1,\n",
              " 'for you .': 207,\n",
              " 'you . I': 38,\n",
              " '. I own': 13,\n",
              " 'I own the': 7,\n",
              " 'own the DVD': 4,\n",
              " 'the DVD and': 26,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdxuVwZepX6U",
        "outputId": "10cc4fe1-63b5-44a3-9a7f-40225d237a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#Creating a dataframe of bigram and trigram where last word from trigram is 'target' for the bigram model. 'Cnt' represent count of trigrams\n",
        "markov = pd.DataFrame.from_dict(dictionary,orient=\"index\")\n",
        "markov = markov.reset_index()\n",
        "markov.columns = ['trigrams','cnt']\n",
        "markov['bigram'] =markov.trigrams.apply(lambda x: \" \".join(x.split(' ')[:2]))\n",
        "markov['target'] =markov.trigrams.apply(lambda x: x.split(' ')[2])\n",
        "markov\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trigrams</th>\n",
              "      <th>cnt</th>\n",
              "      <th>bigram</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Movies have put</td>\n",
              "      <td>1</td>\n",
              "      <td>Movies have</td>\n",
              "      <td>put</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>have put me</td>\n",
              "      <td>2</td>\n",
              "      <td>have put</td>\n",
              "      <td>me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>put me to</td>\n",
              "      <td>17</td>\n",
              "      <td>put me</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>me to sleep</td>\n",
              "      <td>15</td>\n",
              "      <td>me to</td>\n",
              "      <td>sleep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to sleep before</td>\n",
              "      <td>2</td>\n",
              "      <td>to sleep</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3536997</th>\n",
              "      <td>have produced yet</td>\n",
              "      <td>1</td>\n",
              "      <td>have produced</td>\n",
              "      <td>yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3536998</th>\n",
              "      <td>produced yet another</td>\n",
              "      <td>1</td>\n",
              "      <td>produced yet</td>\n",
              "      <td>another</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3536999</th>\n",
              "      <td>yet another snickering</td>\n",
              "      <td>1</td>\n",
              "      <td>yet another</td>\n",
              "      <td>snickering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537000</th>\n",
              "      <td>another snickering embarassment</td>\n",
              "      <td>1</td>\n",
              "      <td>another snickering</td>\n",
              "      <td>embarassment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537001</th>\n",
              "      <td>snickering embarassment .</td>\n",
              "      <td>1</td>\n",
              "      <td>snickering embarassment</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3537002 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                trigrams  ...        target\n",
              "0                        Movies have put  ...           put\n",
              "1                            have put me  ...            me\n",
              "2                              put me to  ...            to\n",
              "3                            me to sleep  ...         sleep\n",
              "4                        to sleep before  ...        before\n",
              "...                                  ...  ...           ...\n",
              "3536997                have produced yet  ...           yet\n",
              "3536998             produced yet another  ...       another\n",
              "3536999           yet another snickering  ...    snickering\n",
              "3537000  another snickering embarassment  ...  embarassment\n",
              "3537001        snickering embarassment .  ...             .\n",
              "\n",
              "[3537002 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2lpWOX6L1PR",
        "outputId": "ba5c5d36-2ef8-4fae-ee7e-2e9a66ecd020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#Count of each bigram\n",
        "inp_cnt = pd.DataFrame(markov.groupby(\"bigram\",as_index=False)[\"cnt\"].sum())\n",
        "inp_cnt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigram</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\b\b\b\bA Turkish</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\t \"</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\t Alex</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\t Also</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\t As</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325621</th>\n",
              "      <td>… although</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325622</th>\n",
              "      <td>… but</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325623</th>\n",
              "      <td>₤100 per</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325624</th>\n",
              "      <td>₤250,000 in</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325625</th>\n",
              "      <td> \"</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1325626 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                bigram  cnt\n",
              "0        \b\b\b\bA Turkish    1\n",
              "1                 \\t \"    2\n",
              "2              \\t Alex    1\n",
              "3              \\t Also    1\n",
              "4                \\t As    3\n",
              "...                ...  ...\n",
              "1325621     … although    1\n",
              "1325622          … but    3\n",
              "1325623       ₤100 per    1\n",
              "1325624    ₤250,000 in    1\n",
              "1325625             \"    2\n",
              "\n",
              "[1325626 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-sZGTKHL12T"
      },
      "source": [
        "## Q.1 1) B Change the output appropriately in ‘Simple Sentiment Analysis.ipynb’ to build an LSTM based language model. Plot the training performance as a function of epochs/iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHnu9qCXL7Zl"
      },
      "source": [
        "SEED=1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t4byXlDQGwm"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVyk27oQG0D",
        "outputId": "efad103b-7be7-403a-f256-0b376e1ec774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y5Di0qzQG3b",
        "outputId": "dae72824-47ca-4d74-a001-1fa55b9beeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['The', 'year', '2005', 'saw', 'no', 'fewer', 'than', '3', 'filmed', 'productions', 'of', 'H.', 'G.', 'Wells', \"'\", 'great', 'novel', ',', '\"', 'War', 'of', 'the', 'Worlds', '\"', '.', 'This', 'is', 'perhaps', 'the', 'least', 'well', '-', 'known', 'and', 'very', 'probably', 'the', 'best', 'of', 'them', '.', 'No', 'other', 'version', 'of', 'WotW', 'has', 'ever', 'attempted', 'not', 'only', 'to', 'present', 'the', 'story', 'very', 'much', 'as', 'Wells', 'wrote', 'it', ',', 'but', 'also', 'to', 'create', 'the', 'atmosphere', 'of', 'the', 'time', 'in', 'which', 'it', 'was', 'supposed', 'to', 'take', 'place', ':', 'the', 'last', 'year', 'of', 'the', '19th', 'Century', ',', '1900', '\\x85 ', 'using', 'Wells', \"'\", 'original', 'setting', ',', 'in', 'and', 'near', 'Woking', ',', 'England.<br', '/><br', '/>IMDb', 'seems', 'unfriendly', 'to', 'what', 'they', 'regard', 'as', '\"', 'spoilers', '\"', '.', 'That', 'might', 'apply', 'with', 'some', 'films', ',', 'where', 'the', 'ending', 'might', 'actually', 'be', 'a', 'surprise', ',', 'but', 'with', 'regard', 'to', 'one', 'of', 'the', 'most', 'famous', 'novels', 'in', 'the', 'world', ',', 'it', 'seems', 'positively', 'silly', '.', 'I', 'have', 'no', 'sympathy', 'for', 'people', 'who', 'have', 'neglected', 'to', 'read', 'one', 'of', 'the', 'seminal', 'works', 'in', 'English', 'literature', ',', 'so', 'let', \"'s\", 'get', 'right', 'to', 'the', 'chase', '.', 'The', 'aliens', 'are', 'destroyed', 'through', 'catching', 'an', 'Earth', 'disease', ',', 'against', 'which', 'they', 'have', 'no', 'immunity', '.', 'If', 'that', \"'s\", 'a', 'spoiler', ',', 'so', 'be', 'it', ';', 'after', 'a', 'book', 'and', '3', 'other', 'films', '(', 'including', 'the', '1953', 'classic', ')', ',', 'you', 'ought', 'to', 'know', 'how', 'this', 'ends.<br', '/><br', '/>This', 'film', ',', 'which', 'follows', 'Wells', \"'\", 'plot', 'in', 'the', 'main', ',', 'is', 'also', 'very', 'cleverly', 'presented', '\\x96', 'in', 'a', 'way', 'that', 'might', 'put', 'many', 'viewers', 'off', 'due', 'to', 'their', 'ignorance', 'of', 'late', '19th', '/', 'early', '20th', 'Century', 'photography', '.', 'Although', 'filmed', 'in', 'a', 'widescreen', 'aspect', ',', 'the', 'film', 'goes', 'to', 'some', 'lengths', 'to', 'give', 'an', 'impression', 'of', 'contemporaneity', '.', 'The', 'general', 'coloration', 'of', 'skin', 'and', 'clothes', 'display', 'a', 'sepia', 'tint', 'often', 'found', 'in', 'old', 'photographs', '(', 'rather', 'than', 'black', ')', '.', 'Colors', 'are', 'often', 'reminiscent', 'of', 'hand', '-', 'tinting', '.', 'At', 'other', 'times', ',', 'colors', 'are', 'washed', 'out', '.', 'These', 'variations', 'are', 'typical', 'of', 'early', 'films', ',', 'which', 'did', \"n't\", 'use', 'standardized', 'celluloid', 'stock', 'and', 'therefore', 'presented', 'a', 'good', 'many', 'changes', 'in', 'print', 'quality', ',', 'even', 'going', 'from', 'black', '/', 'white', 'to', 'sepia', '/', 'white', 'to', 'blue', '/', 'white', 'to', 'reddish', '/', 'white', 'and', 'so', 'on', '\\x96', 'as', 'you', \"'ll\", 'see', 'on', 'occasion', 'here', '.', 'The', 'special', 'effects', 'are', 'deliberately', 'retrograde', ',', 'of', 'a', 'sort', 'seen', 'even', 'as', 'late', 'as', 'the', '1920s', '\\x96', 'and', 'yet', 'the', 'Martians', 'and', 'their', 'machines', 'are', 'very', 'much', 'as', 'Wells', 'described', 'them', 'and', 'have', 'a', 'more', 'nearly', 'realistic', '\"', 'feel', '\"', '.', 'Some', 'of', 'effects', 'are', 'really', 'awkward', '\\x96', 'such', 'as', 'the', 'destruction', 'of', 'Big', 'Ben', '.', 'The', 'acting', 'is', 'often', 'more', 'in', 'the', 'style', 'of', 'that', 'period', 'than', 'ours', '.', 'Some', 'aspects', 'of', 'Victorian', 'dress', 'may', 'appear', 'odd', ',', 'particularly', 'the', 'use', 'of', 'pomade', 'or', 'brilliantine', 'on', 'head', 'and', 'facial', 'hair.<br', '/><br', '/>This', 'film', 'is', 'the', 'only', 'one', 'that', 'follows', 'with', 'some', 'closeness', 'Wells', \"'\", 'original', 'narrative', '\\x96', 'as', 'has', 'been', 'noted', '.', 'Viewers', 'may', 'find', 'it', 'informative', 'to', 'note', 'plot', 'details', 'that', 'appear', 'here', 'that', 'are', 'occasionally', 'retained', 'in', 'other', 'versions', 'of', 'the', 'story', '.', 'Wells', \"'\", 'description', 'of', 'the', 'Martians', '\\x96', 'a', 'giant', 'head', 'mounted', 'on', 'numerous', 'tentacles', '\\x96', 'is', 'effectively', 'portrayed', '.', 'When', 'the', 'Martian', 'machines', 'appear', ',', 'about', 'an', 'hour', 'into', 'the', 'film', ',', 'they', 'too', 'give', 'a', 'good', 'impression', 'of', 'how', 'Wells', 'described', 'them', '.', 'Both', 'Wells', 'and', 'this', 'film', 'do', 'an', 'excellent', 'job', 'of', 'portraying', 'the', 'progress', 'of', 'the', 'Martians', 'from', 'the', 'limited', 'perspective', '(', 'primarily', ')', 'of', 'rural', 'England', '\\x96', 'plus', 'a', 'few', 'scenes', 'in', 'London', '(', 'involving', 'the', 'Narrator', \"'s\", 'brother', ')', '.', 'The', 'director', 'is', 'unable', 'to', 'resist', 'showing', 'the', 'destruction', 'of', 'a', 'major', 'landmark', '(', 'Big', 'Ben', ')', ',', 'but', 'at', 'least', 'does', \"n't\", 'dwell', 'unduly', 'on', 'the', 'devastation', 'of', 'London.<br', '/><br', '/>The', 'victory', 'of', 'the', 'Martians', 'is', 'hardly', 'a', 'surprise', ',', 'despite', 'the', 'destruction', 'by', 'cannon', 'of', 'some', 'of', 'their', 'machines', '.', 'The', 'Narrator', ',', 'traveling', 'about', 'to', 'seek', 'escape', ',', 'sees', 'much', 'of', 'what', 'Wells', 'terms', '\"', 'the', 'rout', 'of', 'Mankind', '\"', '.', 'He', 'encounters', 'a', 'curate', 'endowed', 'with', 'the', 'Victorian', 'affliction', 'of', 'a', 'much', 'too', 'precious', 'and', 'nervous', 'personality', '.', 'They', 'eventually', 'find', 'themselves', 'on', 'the', 'very', 'edge', 'of', 'a', 'Martian', 'nest', ',', 'where', 'they', 'discover', 'an', 'awful', 'fact', ':', 'the', 'Martians', 'are', 'shown', 'to', 'be', 'vampires', 'who', 'consume', 'their', 'prey', 'alive', 'in', 'a', 'very', 'effective', 'scene', '.', 'Wells', 'adds', 'that', 'after', 'eating', 'they', 'set', 'up', '\"', 'a', 'prolonged', 'and', 'cheerful', 'hooting', '\"', '.', 'The', 'Narrator', 'finally', 'is', 'obliged', 'to', 'beat', 'senseless', 'the', 'increasingly', 'hysterical', 'curate', '\\x96', 'who', 'revives', 'just', 'as', 'the', 'Martians', 'drag', 'him', 'off', 'to', 'the', 'larder', '(', 'cheers', 'from', 'the', 'gallery', ';', 'British', 'curates', 'are', 'so', 'often', 'utterly', 'insufferable).<br', '/><br', '/>This', 'film', 'lasts', 'almost', '3', 'hours', ',', 'going', 'through', 'Wells', \"'\", 'story', 'in', 'welcome', 'detail', '.', 'It', \"'s\", 'about', 'time', 'the', 'author', 'got', 'his', 'due', '\\x96', 'in', 'a', 'compelling', 'presentation', 'that', 'builds', 'in', 'dramatic', 'impact', '.', 'A', 'word', 'about', 'the', 'acting', ':', 'Do', \"n't\", 'expect', 'award', '-', 'winning', 'performances', '.', 'They', \"'re\", 'not', 'bad', ',', 'however', ',', 'the', 'actors', 'are', 'earnest', 'and', 'they', 'grow', 'on', 'you', '.', 'Most', 'of', 'them', ',', 'however', ',', 'have', 'had', 'very', 'abbreviated', 'film', 'careers', ',', 'often', 'only', 'in', 'this', 'film', '.', 'The', 'Narrator', 'is', 'played', 'by', 'hunky', 'Anthony', 'Piana', ',', 'in', 'his', '2nd', 'film', '.', 'The', 'Curate', 'is', 'John', 'Kaufman', '\\x96', 'also', 'in', 'his', '2nd', 'film', 'as', 'an', 'actor', 'but', 'who', 'has', 'had', 'more', 'experience', 'directing', '.', 'The', 'Brother', '(', '\"', 'Henderson', '\"', ')', 'is', 'played', 'with', 'some', 'conviction', 'by', 'W.', 'Bernard', 'Bauman', 'in', 'his', 'first', 'film', '.', 'The', 'Artilleryman', ',', 'the', 'only', 'other', 'sizable', 'part', ',', 'is', 'played', 'by', 'James', 'Lathrop', 'in', 'his', 'first', 'film.<br', '/><br', '/>This', 'is', 'overall', 'a', 'splendid', 'film', ',', 'portraying', 'for', 'the', 'first', 'time', 'the', 'War', 'of', 'the', 'Worlds', 'as', 'Wells', 'wrote', 'it', '.', 'Despite', 'its', 'slight', 'defects', ',', 'it', 'is', 'far', 'and', 'away', 'better', 'than', 'any', 'of', 'its', 'hyped', '-', 'up', 'competitors', '.', 'If', 'you', 'want', 'to', 'see', 'H.', 'G.', 'Wells', \"'\", 'War', 'of', 'the', 'Worlds', '\\x96', 'and', 'not', 'some', 'wholly', 'distorted', 'version', 'of', 'it', '\\x96', 'see', 'this', 'film', '!'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3MrdQqvQUPC",
        "outputId": "eabba78f-43a8-413c-e0b9-23d8f09f0b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj51g3fLQUbp"
      },
      "source": [
        "#Taking the 10000 most commonly used words in reviews\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7XlTk1Qgz1",
        "outputId": "1c054016-cc09-4e92-8d61-cdca15d1a87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 10002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PxVGMthQg3V",
        "outputId": "d29201a6-4f37-4e27-9268-4911fad36858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 203067), (',', 192631), ('.', 166305), ('and', 109610), ('a', 109559), ('of', 100576), ('to', 94057), ('is', 76455), ('in', 61626), ('I', 54641), ('it', 53553), ('that', 49552), ('\"', 43965), (\"'s\", 43155), ('this', 42189), ('-', 37006), ('/><br', 35833), ('was', 35360), ('as', 30417), ('with', 29930)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0NoLgYxQzPS",
        "outputId": "8bddc9b2-5d47-4c5a-857f-8b6fac6daf79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oi0-LU7QzRw",
        "outputId": "72724af1-131c-4194-a404-e735628795ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f2447349ae8>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWxQFMTlQzUu"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IeDfXLJCIyV"
      },
      "source": [
        "Defining LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsX0-9dyMqls"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, (hidden,cell) = self.lstm(embedded)\n",
        "        #output, hidden = self.lstm(embedded)\n",
        "        #print(hidden.dtype)\n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))\n",
        "        #return self.fc(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJt4FlCIgPnL"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI7nDetEgPpi",
        "outputId": "a0ca4120-b0a2-40a8-8fb6-75be3b7d0a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 1,367,049 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMie4RlvgPsU"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqI8yRddgPf6"
      },
      "source": [
        "#Function for checking accuracy\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9krfmS9am7qE"
      },
      "source": [
        "#Function for training the model on training data of 17500 reviews\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvqPYjNEmGdw"
      },
      "source": [
        "#Function for evaluating model on evaluation data of 7500 reviews\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shFLQiEAmcZ7"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qOWoS9qQ7-8",
        "outputId": "84ccf331-0e34-48c1-8f63-88a486c4ae1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "N_EPOCHS = 14\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.31%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 48.71%\n",
            "Epoch: 02 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.08%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 48.60%\n",
            "Epoch: 03 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.73%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 48.24%\n",
            "Epoch: 04 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.19%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 48.73%\n",
            "Epoch: 05 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.37%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.20%\n",
            "Epoch: 06 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.30%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.25%\n",
            "Epoch: 07 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.57%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.01%\n",
            "Epoch: 08 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.47%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.44%\n",
            "Epoch: 09 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.60%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.19%\n",
            "Epoch: 10 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.61%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.51%\n",
            "Epoch: 11 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.75%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.56%\n",
            "Epoch: 12 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.45%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 49.93%\n",
            "Epoch: 13 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.63%\n",
            "\t Val. Loss: 0.693 |  Val. Acc: 50.51%\n",
            "Epoch: 14 | Epoch Time: 1m 13s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.88%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTiqduH9IOWT"
      },
      "source": [
        "Plotting the training and evaluation performance as a function\n",
        "of epochs/iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKsH_LY9lCbG",
        "outputId": "a2946512-ba44-45a3-c235-425c9f8ec113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(train_losses,'r',label='Training loss')\n",
        "plt.title(\"Training Performane by EPOCHS - 5000 words 80% training size\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title('Loss by number of Epochs') \n",
        "plt.plot(valid_losses,'b',label='Validation loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XKzPmx8W8qr"
      },
      "source": [
        "##Q 1.2) For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEZ9wom_uEn5"
      },
      "source": [
        "Markov model we went with the trigrams and there was no difficulty in calculating the probabilities and prediction.\n",
        "For LSTM there was an input layer followed by embedding then lstm layer. The input was the tokenized text and output also the tokenized text offsetted by 1 place. Ex) if input is [This,is,my,favorite,movie] then output is [is,my,favorite,movie,<\n",
        "\n",
        "We took 25% training data with max 1000 words with batch size 10 to train our lstm model.\n",
        "If we increased the number of words then the model couldn't be loaded into the gpu as there was insufficient memory\n",
        "If we increased the training data to higher percentage say 50% or 80% then notebook got disconnected from the server randomly. This error didn't go when we tried to run the code as standalone .py file instead of a python notebook\n",
        "When we reduced the batch size to 1 then model took longer time to train and when we increased it to 64 then there was memory error once again. We settled at Batch Size 10\n",
        "Due to this limited training size and words the quality of sentence formed by predicition was affected considerably.The prediction didn't give out a coherent sentence as we would have liked."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3SKGgGAkRnT"
      },
      "source": [
        "**Markov Model**\n",
        "\n",
        "We built Markov model with the trigrams. Initially, We have custom function is used to generate trigrams which is passed as the preprocessing argument in the data.field function and transforms each review into trigrams. Also, freqs function is used to get the count of each trigram and converted to a dictionary. Data is further manipulated to calculate probabilties and predictions are made my smapling these probabilities\n",
        "\n",
        "\n",
        "\n",
        "**LSTM Model**\n",
        "\n",
        "For LSTM model, there was an input layer followed by embedding then lstm layer and at the end output layer.\n",
        "\n",
        "The key design choices made are for the following parameters:\n",
        "1. Batch Size\n",
        "Use of smaller batch size are shown to have faster convergence to good solution because smaller batch size allows the model to to “start learning before having to see all the data. If we increase the batch size to 100 we have memory issue and hence for above two reasons we took 10 to be the appropriate batch size.\n",
        "\n",
        "2. Embedding Dimension\n",
        "In case of word embedding the number of pairwise equidistant words of the corpus vocabulary gives a lower bound on the number of dimensions and going below this bound results in degradation of quality of learned word embeddings.\n",
        "We have used word embedding of 100 Dimension as if we go lower value like 50 Dimension we would not be able to differentiate words whereas at higher dimension like 300 there will be even no relation between similar words like good and amazing making it reducing the accuracy on training data.\n",
        "\n",
        "3. Vocabulary Size\n",
        "The choice of vocabulary size is in line with the choice of embedding dimension. Plus, the higher number of word in the vocabulary allows the model to be trained better to with more words to give better performance for the model. Here we have chose the vocabulary size to be 10000. If we go beyond this number say 20000 then there was memory issue. Also training the modelk on lower vocabulary would train our model less accurately. So, we used vocabulary size of 10000.\n",
        "\n",
        "4. Epoch\n",
        "We used 35% (17500 reviews) of our data as training data and 15% validation data. Remaining 50% is used as test data. \n",
        "\n",
        "We have used the Early Stopping approach to determine the number of epochs: divide data in three data sets, training, validation and evaluation. Train each network along a sufficient number of epochs to see the training Mean Squared Error to be stuck in a minimum. The network for the epoch with the minimum validation MSE and once the loss has reached the value after which it was not changing much is selected for the evaluation process. Here it is coming to be 14\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyWpWSO9jMo7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgpjbA40XDox"
      },
      "source": [
        "##Q 1. 3) For each model, starting with the phrase ”My favorite movie ”, sample the next few words and create a 20 word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABLkvRLmDFrv"
      },
      "source": [
        "##Markov Model Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxBlff5arob",
        "outputId": "283fa38f-9ace-4120-a22d-f7492c28d25c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "\n",
        "def generate_sentence(sentence,no_preds,prediction_length):\n",
        "    prediction_length -= len(sentence.split(\" \")) \n",
        "    for j in range(0,no_preds):\n",
        "        pred_sentence = sentence\n",
        "        bi = \" \".join(pred_sentence.split(\" \")[1:])\n",
        "        for i in range(0,prediction_length):\n",
        "            poss_pred = np.array(markov.target[markov.bigram==bi])\n",
        "            scores = np.array(markov.cnt[markov.bigram==bi])\n",
        "            length = inp_cnt.cnt[inp_cnt.bigram==bi]\n",
        "            probs = scores/length.iloc[0]\n",
        "            pred = random.choice(list(enumerate(probs)))[0]\n",
        "            pred = poss_pred[pred]\n",
        "            pred_sentence = pred_sentence + \" \" + pred\n",
        "            bi = \" \".join([bi.split(\" \")[1],pred])\n",
        "        print(\"Markov model Generation \"+str(j+1)+\": \",pred_sentence+\"\\n\")\n",
        "generate_sentence(\"my favorite movie\",5,20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Markov model Generation 1:  my favorite movie as there would ( rightfully ) be topless , but starts to seeing Michael Madsen ! (\n",
            "\n",
            "Markov model Generation 2:  my favorite movie a rating less than 10 lines so , too ; people who ultimately married the three suspects\n",
            "\n",
            "Markov model Generation 3:  my favorite movie ever probably , one being tested , I disliked her so succeed . Of coming out soon\n",
            "\n",
            "Markov model Generation 4:  my favorite movie ) types of bad Sci - Fi entertainment , this not so much anger toward McCoy and\n",
            "\n",
            "Markov model Generation 5:  my favorite movie ( sort of wimp who was hanged , good taste public they will leave Mathieu confused ,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ-Nj6J_HcBx"
      },
      "source": [
        "##LSTM Model Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOPvsxd3ENx_",
        "outputId": "37a9aae3-2872-4076-9193-ac9e4247c684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#LSTM Prediction\n",
        "def predict_LTSM(model, next_words=17):\n",
        "  #x =torch.tensor([[TEXT.vocab.stoi[\"my\"]],[TEXT.vocab.stoi[\"favorite\"]],[TEXT.vocab.stoi[\"movie\"]]],device=\"cuda:0\")\n",
        "  text='my favorite movie'\n",
        "  words = text.split(' ')\n",
        "  #TEXT.vocab.stoi['movie']\n",
        "\n",
        "  for i in range(0, next_words):\n",
        "    x = torch.tensor([[TEXT.vocab.stoi[w] for w in words[i:]]],device=\"cuda:0\")\n",
        "    \n",
        "    y_pred = model(x)#, (state_h, state_c))\n",
        "    #print(y_pred[0][-1])\n",
        "    last_word_logits = y_pred[0]#[-1]\n",
        "    \n",
        "    p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()#.detach().numpy()\n",
        "  \n",
        "    word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "    words.append(TEXT.vocab.itos[word_index])\n",
        "\n",
        "  return \" \".join(words)\n",
        "\n",
        "for j in range(0,5):\n",
        "  #print(predict_LTSM(model))\n",
        "  print(\"Generated Review\"+str(j+1)+\": \",predict_LTSM(model)+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Review 1:  my favorite movie rental on this consisted of 4 , \" Deerhunter \" parody \" angle as an encryption -\n",
            "\n",
            "Generated Review 2:  my favorite movie from about five times and most brilliant stroke . She glides rather than organic . For 1980\n",
            "\n",
            "Generated Review 3:  my favorite movie almost 10 years back called DINNER RUSH is highly satisfactory , and obviously runs into Connie first\n",
            "\n",
            "Generated Review 4:  my favorite movie of yours was a route and reach Dawson only to turn gradually into psychological uneasiness eventually blossoming\n",
            "\n",
            "Generated Review 5:  my favorite movie he plays his same old rubbish time after 45 years , nor ordinary . < hr >\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Pj7OOOMlOq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWs1tMeMleh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEpBEjB5QGN9"
      },
      "source": [
        "#Sequence to Sequence Model for Translation (40pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kv6vIUyQP7E"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddwgtrlkp_8o"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvE6BwUNvtPv"
      },
      "source": [
        "## Reading Sentence Translation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpsQ31FUq1tc"
      },
      "source": [
        "data_file = \"/content/drive/My Drive/lang/mar-eng/mar.txt\"\n",
        "#data_file = \"/content/drive/My Drive/lang/spa-eng/spa.txt\"\n",
        "f = open(data_file, 'r')\n",
        "data_lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUHULPKuhVP"
      },
      "source": [
        "language_x,language_y = [],[]\n",
        "for line in data_lines:\n",
        "  sentence = line.split(\"\\t\")\n",
        "  language_x.append(sentence[1].strip())\n",
        "  language_y.append(sentence[0].strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WobMh-Fi03uT",
        "outputId": "e97889ac-e8ba-4d67-afa6-a29fddc7562a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "language_x[170],language_y[170]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('मी गरीब आहे.', \"I'm poor.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YUSd0D5XWVW",
        "outputId": "4b5d791b-545b-44ef-ce17-243c3bc26fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(language_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdJCGyvzTN_"
      },
      "source": [
        "## Tokenizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfzY6RVWyNij"
      },
      "source": [
        "tokenizer_X = Tokenizer(oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer_X.word_index['<pad>'] = 0\n",
        "tokenizer_X.index_word[0] = '<pad>'\n",
        "tokenizer_X.fit_on_texts(language_x)\n",
        "sequence_x = tokenizer_X.texts_to_sequences(language_x)\n",
        "\n",
        "tokenizer_y = Tokenizer(oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer_y.word_index['<pad>'] = 0\n",
        "tokenizer_y.index_word[0] = '<pad>'\n",
        "tokenizer_y.fit_on_texts(language_y)\n",
        "sequence_y = tokenizer_y.texts_to_sequences(language_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUOEGbU20M0i",
        "outputId": "68bd3c17-a15e-417e-cf11-79ecc21e3e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequence_x[170],sequence_y[170]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3, 657, 2], [29, 598])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgaAH4K0z0wQ"
      },
      "source": [
        "## Padding Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY-dWoHgy3Qu"
      },
      "source": [
        "sequence_x_padded = pad_sequences(sequence_x, padding='post')\n",
        "sequence_y_padded = pad_sequences(sequence_y, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i-QyUOuyp0A",
        "outputId": "4377165d-153c-4109-9127-7b5959304a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequence_x_padded[40000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    8,  4119,    43,  5105,    91, 13517,    43,  4627,   264,\n",
              "         934,    63,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqauy9Ij2aat",
        "outputId": "3ead8a4e-27e2-49be-9655-dc273c9b8711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sequence_y_padded[40000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 36,  53, 655, 499,  19,   8, 323,  81, 588,  40,   8, 334,  81,\n",
              "       759,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nExSzSjz3AH"
      },
      "source": [
        "## Splitting into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "islnPsTtytSR"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( sequence_x_padded, sequence_y_padded, test_size=0.05, random_state=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTkA9pKs2fVa",
        "outputId": "30b0d1c5-2e87-4083-8bc7-2a2b38e45b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   3, 1038,  145, ...,    0,    0,    0],\n",
              "       [   6,  505,   37, ...,    0,    0,    0],\n",
              "       [  59,  782, 1970, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  40, 6279, 9100, ...,    0,    0,    0],\n",
              "       [   9,   62,  184, ...,    0,    0,    0],\n",
              "       [3862, 2932,    2, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkzZtdUx28tQ",
        "outputId": "e1d557b4-07e1-49bc-bd98-680a3aa60537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,  77,   6, ...,   0,   0,   0],\n",
              "       [412,   2, 138, ...,   0,   0,   0],\n",
              "       [112,  71,  41, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [ 14, 305,  43, ...,   0,   0,   0],\n",
              "       [  4,  45,  56, ...,   0,   0,   0],\n",
              "       [ 39,   7,   8, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNiTksyW2-Ju",
        "outputId": "e1ff3b4c-5a8e-4077-c030-c97cd7cfe75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tokenizer_X.sequences_to_texts([X_train[0]])[0].replace(\"<unk>\",\"\").strip())\n",
        "print(tokenizer_y.sequences_to_texts([y_train[0]])[0].replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "मी दुकानात गेलो\n",
            "i went to the shop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J47yso29BhD"
      },
      "source": [
        "## Creating LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broufTQ738fT",
        "outputId": "9eea67bf-4e26-465e-b37f-d559af2ff49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x_vocab_size = len(tokenizer_X.word_index) + 1\n",
        "y_vocab_size = len(tokenizer_y.word_index) + 1\n",
        "x_length = len(X_train[0])\n",
        "y_length = len(y_train[0])\n",
        "\n",
        "print(\"Vocab size of original language: \",x_vocab_size)\n",
        "print(\"Vocab size of translated language: \",y_vocab_size)\n",
        "print(\"Vector length of each sentence in original language: \",x_length)\n",
        "print(\"Vector length of each sentence i translated language: \",y_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size of original language:  13786\n",
            "Vocab size of translated language:  5857\n",
            "Vector length of each sentence in original language:  35\n",
            "Vector length of each sentence i translated language:  35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrFoj4jr4ESw"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(x_vocab_size, 100, input_length=x_length, mask_zero=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(RepeatVector(y_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(y_vocab_size, activation='softmax'))\n",
        "rms = RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERg4XEnT4-3h",
        "outputId": "f732db35-1be9-4bc0-9999-82f054ba3fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 35, 100)           1378600   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 35, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 35, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 35, 5857)          591557    \n",
            "=================================================================\n",
            "Total params: 2,130,957\n",
            "Trainable params: 2,130,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRf1SCI8-m17"
      },
      "source": [
        "## Fitting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz8GgSJLL5-U"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlTh2cagRHSa",
        "outputId": "e91e135f-7969-4000-d7d7-861bf7008a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc66d38e198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhtMzcdl-l-c",
        "outputId": "e37fb8d0-1b2d-4c03-aa4f-b4bd86443a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=64, epochs=50, validation_split=0.1,callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2770\n",
            "Epoch 00001: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 21s 39ms/step - loss: 0.2770 - val_loss: 0.5669\n",
            "Epoch 2/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2736\n",
            "Epoch 00002: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2736 - val_loss: 0.5710\n",
            "Epoch 3/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2703\n",
            "Epoch 00003: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 36ms/step - loss: 0.2703 - val_loss: 0.5666\n",
            "Epoch 4/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2666\n",
            "Epoch 00004: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2666 - val_loss: 0.5712\n",
            "Epoch 5/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2639\n",
            "Epoch 00005: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2639 - val_loss: 0.5677\n",
            "Epoch 6/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2604\n",
            "Epoch 00006: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2604 - val_loss: 0.5703\n",
            "Epoch 7/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2571\n",
            "Epoch 00007: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2571 - val_loss: 0.5753\n",
            "Epoch 8/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2547\n",
            "Epoch 00008: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2547 - val_loss: 0.5728\n",
            "Epoch 9/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.2513\n",
            "Epoch 00009: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2513 - val_loss: 0.5743\n",
            "Epoch 10/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2482\n",
            "Epoch 00010: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 36ms/step - loss: 0.2482 - val_loss: 0.5727\n",
            "Epoch 11/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2452\n",
            "Epoch 00011: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 37ms/step - loss: 0.2452 - val_loss: 0.5748\n",
            "Epoch 12/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2424\n",
            "Epoch 00012: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2424 - val_loss: 0.5764\n",
            "Epoch 13/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2395\n",
            "Epoch 00013: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2395 - val_loss: 0.5753\n",
            "Epoch 14/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2361\n",
            "Epoch 00014: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2361 - val_loss: 0.5781\n",
            "Epoch 15/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2337\n",
            "Epoch 00015: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2337 - val_loss: 0.5776\n",
            "Epoch 16/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2308\n",
            "Epoch 00016: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2308 - val_loss: 0.5782\n",
            "Epoch 17/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2279\n",
            "Epoch 00017: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2279 - val_loss: 0.5774\n",
            "Epoch 18/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2253\n",
            "Epoch 00018: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2253 - val_loss: 0.5797\n",
            "Epoch 19/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2230\n",
            "Epoch 00019: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2230 - val_loss: 0.5862\n",
            "Epoch 20/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.2206\n",
            "Epoch 00020: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2206 - val_loss: 0.5823\n",
            "Epoch 21/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2182\n",
            "Epoch 00021: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2182 - val_loss: 0.5846\n",
            "Epoch 22/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2158\n",
            "Epoch 00022: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2158 - val_loss: 0.5872\n",
            "Epoch 23/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2136\n",
            "Epoch 00023: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2136 - val_loss: 0.5845\n",
            "Epoch 24/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2114\n",
            "Epoch 00024: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2114 - val_loss: 0.5885\n",
            "Epoch 25/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2093\n",
            "Epoch 00025: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2093 - val_loss: 0.5870\n",
            "Epoch 26/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.2069\n",
            "Epoch 00026: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 37ms/step - loss: 0.2069 - val_loss: 0.5886\n",
            "Epoch 27/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2049\n",
            "Epoch 00027: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 36ms/step - loss: 0.2049 - val_loss: 0.5917\n",
            "Epoch 28/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2028\n",
            "Epoch 00028: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2028 - val_loss: 0.5933\n",
            "Epoch 29/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.2006\n",
            "Epoch 00029: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.2006 - val_loss: 0.5936\n",
            "Epoch 30/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1986\n",
            "Epoch 00030: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1986 - val_loss: 0.5955\n",
            "Epoch 31/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1972\n",
            "Epoch 00031: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1972 - val_loss: 0.5957\n",
            "Epoch 32/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1953\n",
            "Epoch 00032: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.1953 - val_loss: 0.5980\n",
            "Epoch 33/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1936\n",
            "Epoch 00033: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1936 - val_loss: 0.5994\n",
            "Epoch 34/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1916\n",
            "Epoch 00034: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1916 - val_loss: 0.6018\n",
            "Epoch 35/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1898\n",
            "Epoch 00035: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1898 - val_loss: 0.5999\n",
            "Epoch 36/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1881\n",
            "Epoch 00036: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.1881 - val_loss: 0.6015\n",
            "Epoch 37/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1865\n",
            "Epoch 00037: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1865 - val_loss: 0.6054\n",
            "Epoch 38/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1852\n",
            "Epoch 00038: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1852 - val_loss: 0.6096\n",
            "Epoch 39/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1835\n",
            "Epoch 00039: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1835 - val_loss: 0.6048\n",
            "Epoch 40/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1817\n",
            "Epoch 00040: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1817 - val_loss: 0.6054\n",
            "Epoch 41/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1805\n",
            "Epoch 00041: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1805 - val_loss: 0.6140\n",
            "Epoch 42/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1786\n",
            "Epoch 00042: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1787 - val_loss: 0.6135\n",
            "Epoch 43/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1773\n",
            "Epoch 00043: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1773 - val_loss: 0.6152\n",
            "Epoch 44/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1758\n",
            "Epoch 00044: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1758 - val_loss: 0.6128\n",
            "Epoch 45/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1749\n",
            "Epoch 00045: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1749 - val_loss: 0.6149\n",
            "Epoch 46/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1737\n",
            "Epoch 00046: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1737 - val_loss: 0.6181\n",
            "Epoch 47/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1725\n",
            "Epoch 00047: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 35ms/step - loss: 0.1725 - val_loss: 0.6199\n",
            "Epoch 48/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.1709\n",
            "Epoch 00048: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 37ms/step - loss: 0.1709 - val_loss: 0.6219\n",
            "Epoch 49/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1697\n",
            "Epoch 00049: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 19s 36ms/step - loss: 0.1697 - val_loss: 0.6263\n",
            "Epoch 50/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.1683\n",
            "Epoch 00050: saving model to /content/drive/My Drive/lang/Checkpt_spa/cp.ckpt\n",
            "537/537 [==============================] - 20s 37ms/step - loss: 0.1682 - val_loss: 0.6212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PjLaBU5AMJf"
      },
      "source": [
        "## Validating Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z9NO4-Gectn"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PQBggsG-2n5"
      },
      "source": [
        "y_pred_logits = model.predict(X_test[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Zlc_mTBfeI"
      },
      "source": [
        "y_pred=[]\n",
        "for pred in y_pred_logits: \n",
        "  y_pred.append(np.argmax(pred,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC2Ajm62AGNa"
      },
      "source": [
        "y_pred_text = tokenizer_y.sequences_to_texts(y_pred)\n",
        "y_test_text = tokenizer_y.sequences_to_texts(y_test[:200])\n",
        "X_test_text = tokenizer_X.sequences_to_texts(X_test[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68F9ykZYfE_M",
        "outputId": "2aa97d46-d524-4b7b-933c-970a2e360e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred_text[145]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we made him made <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWDoExI6ARnb",
        "outputId": "faae663b-ad92-44e6-8436-b990c1ed235e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Original Sentence: \",X_test_text[145].replace(\"<unk>\",\"\").strip())\n",
        "print(\"Predicted Sentence: \",y_pred_text[145].replace(\"<unk>\",\"\").strip())\n",
        "print(\"Actual Sentence: \",y_test_text[145].replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence:  आम्ही त्यांना पळवून लावलं\n",
            "Predicted Sentence:  we made him made\n",
            "Actual Sentence:  we drove them out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is0nxVZFp6NC"
      },
      "source": [
        "# PART 2 (Embedding using GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLed_YgA1yEy"
      },
      "source": [
        "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split( sequence_y_padded, sequence_x_padded, test_size=0.05, random_state=12)\n",
        "tokenizer_X_g = tokenizer_y\n",
        "tokenizer_y_g = tokenizer_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eouOAze34Bsn",
        "outputId": "b03c8dfb-a951-42a4-a373-a433d11a610b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x_vocab_size_g = len(tokenizer_X_g.word_index) + 1\n",
        "y_vocab_size_g = len(tokenizer_y_g.word_index) + 1\n",
        "x_length_g = len(X_train_g[0])\n",
        "y_length_g = len(y_train_g[0])\n",
        "\n",
        "print(\"Vocab size of original language: \",x_vocab_size_g)\n",
        "print(\"Vocab size of translated language: \",y_vocab_size_g)\n",
        "print(\"Vector length of each sentence in original language: \",x_length_g)\n",
        "print(\"Vector length of each sentence i translated language: \",y_length_g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size of original language:  5857\n",
            "Vocab size of translated language:  13786\n",
            "Vector length of each sentence in original language:  35\n",
            "Vector length of each sentence i translated language:  35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IUgx67yBMBm",
        "outputId": "93b8323c-c528-4c19-ef72-c0f90e4f231f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/glove.6B/', 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suptDo1E1Kb-"
      },
      "source": [
        "word_i= tokenizer_X_g.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZmCIpTqdiI"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_i) + 1, 100))\n",
        "for word, i in word_i.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-9jbvAx_vM_"
      },
      "source": [
        "## Model with GloVe Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilQmsD4_4mCt"
      },
      "source": [
        "model_g = Sequential()\n",
        "model_g.add(Embedding(len(word_i) + 1,\n",
        "                            100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=x_length_g,\n",
        "                            trainable=False))\n",
        "model_g.add(LSTM(100))\n",
        "model_g.add(RepeatVector(y_length_g))\n",
        "model_g.add(LSTM(100, return_sequences=True))\n",
        "model_g.add(Dense(y_vocab_size_g, activation='softmax'))\n",
        "rms = RMSprop(lr=0.001)\n",
        "model_g.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rMcViRL6Lt1",
        "outputId": "6dbe7e89-e19c-463e-9359-3ab76998ec1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model_g.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 35, 100)           585700    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 35, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 35, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 35, 13786)         1392386   \n",
            "=================================================================\n",
            "Total params: 2,138,886\n",
            "Trainable params: 1,553,186\n",
            "Non-trainable params: 585,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Vmo4e3R_Bx"
      },
      "source": [
        "checkpoint_path_g = \"/content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\"\n",
        "checkpoint_dir_g = os.path.dirname(checkpoint_path_g)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback_g = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_g,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3-G4-tOShhg",
        "outputId": "187d41bf-9a8f-489d-c809-69ede25589b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path_g)\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir_g)\n",
        "model_g.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc5fe7ec9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQS9zPSa5DYv",
        "outputId": "c15a8b8f-39c6-450c-89c7-5d06d4b762d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_g = model_g.fit(X_train_g, y_train_g, batch_size=64, epochs=50, validation_split=0.1,callbacks=[cp_callback_g])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 1.2596\n",
            "Epoch 00001: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 33s 61ms/step - loss: 1.2596 - val_loss: 0.9618\n",
            "Epoch 2/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.9481\n",
            "Epoch 00002: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.9481 - val_loss: 0.9302\n",
            "Epoch 3/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.9294\n",
            "Epoch 00003: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.9294 - val_loss: 0.9381\n",
            "Epoch 4/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.9159\n",
            "Epoch 00004: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.9159 - val_loss: 0.9120\n",
            "Epoch 5/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8971\n",
            "Epoch 00005: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8971 - val_loss: 0.9041\n",
            "Epoch 6/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8849\n",
            "Epoch 00006: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8849 - val_loss: 0.8817\n",
            "Epoch 7/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8656\n",
            "Epoch 00007: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8656 - val_loss: 0.8742\n",
            "Epoch 8/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8517\n",
            "Epoch 00008: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8517 - val_loss: 0.8491\n",
            "Epoch 9/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8504\n",
            "Epoch 00009: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8504 - val_loss: 0.8470\n",
            "Epoch 10/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8275\n",
            "Epoch 00010: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8275 - val_loss: 0.8321\n",
            "Epoch 11/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8163\n",
            "Epoch 00011: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8163 - val_loss: 0.8260\n",
            "Epoch 12/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.8087\n",
            "Epoch 00012: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.8087 - val_loss: 0.8202\n",
            "Epoch 13/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.7951\n",
            "Epoch 00013: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7952 - val_loss: 0.8054\n",
            "Epoch 14/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7853\n",
            "Epoch 00014: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7853 - val_loss: 0.8014\n",
            "Epoch 15/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7800\n",
            "Epoch 00015: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7800 - val_loss: 0.7995\n",
            "Epoch 16/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7695\n",
            "Epoch 00016: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7695 - val_loss: 0.7857\n",
            "Epoch 17/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7587\n",
            "Epoch 00017: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7587 - val_loss: 0.7814\n",
            "Epoch 18/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7549\n",
            "Epoch 00018: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7549 - val_loss: 0.7825\n",
            "Epoch 19/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7414\n",
            "Epoch 00019: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7414 - val_loss: 0.7716\n",
            "Epoch 20/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7340\n",
            "Epoch 00020: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7340 - val_loss: 0.7624\n",
            "Epoch 21/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7363\n",
            "Epoch 00021: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7363 - val_loss: 0.7603\n",
            "Epoch 22/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7162\n",
            "Epoch 00022: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7162 - val_loss: 0.7481\n",
            "Epoch 23/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7090\n",
            "Epoch 00023: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7090 - val_loss: 0.7542\n",
            "Epoch 24/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.7049\n",
            "Epoch 00024: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7049 - val_loss: 0.7438\n",
            "Epoch 25/50\n",
            "536/537 [============================>.] - ETA: 0s - loss: 0.7015\n",
            "Epoch 00025: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.7015 - val_loss: 0.7466\n",
            "Epoch 26/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6986\n",
            "Epoch 00026: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6986 - val_loss: 0.7453\n",
            "Epoch 27/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6955\n",
            "Epoch 00027: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6955 - val_loss: 0.7446\n",
            "Epoch 28/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6943\n",
            "Epoch 00028: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6943 - val_loss: 0.7415\n",
            "Epoch 29/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6919\n",
            "Epoch 00029: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6919 - val_loss: 0.7408\n",
            "Epoch 30/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6899\n",
            "Epoch 00030: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6899 - val_loss: 0.7466\n",
            "Epoch 31/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6866\n",
            "Epoch 00031: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6866 - val_loss: 0.7408\n",
            "Epoch 32/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6832\n",
            "Epoch 00032: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6832 - val_loss: 0.7412\n",
            "Epoch 33/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6792\n",
            "Epoch 00033: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6792 - val_loss: 0.7374\n",
            "Epoch 34/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6660\n",
            "Epoch 00034: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6660 - val_loss: 0.7282\n",
            "Epoch 35/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6620\n",
            "Epoch 00035: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6620 - val_loss: 0.7272\n",
            "Epoch 36/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6594\n",
            "Epoch 00036: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6594 - val_loss: 0.7313\n",
            "Epoch 37/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6579\n",
            "Epoch 00037: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6579 - val_loss: 0.7270\n",
            "Epoch 38/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6555\n",
            "Epoch 00038: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6555 - val_loss: 0.7311\n",
            "Epoch 39/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6561\n",
            "Epoch 00039: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6561 - val_loss: 0.7325\n",
            "Epoch 40/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6513\n",
            "Epoch 00040: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6513 - val_loss: 0.7264\n",
            "Epoch 41/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6486\n",
            "Epoch 00041: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 60ms/step - loss: 0.6486 - val_loss: 0.7500\n",
            "Epoch 42/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6459\n",
            "Epoch 00042: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6459 - val_loss: 0.7272\n",
            "Epoch 43/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6429\n",
            "Epoch 00043: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6429 - val_loss: 0.7339\n",
            "Epoch 44/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6399\n",
            "Epoch 00044: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6399 - val_loss: 0.7196\n",
            "Epoch 45/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6367\n",
            "Epoch 00045: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6367 - val_loss: 0.7225\n",
            "Epoch 46/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6338\n",
            "Epoch 00046: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6338 - val_loss: 0.7175\n",
            "Epoch 47/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6308\n",
            "Epoch 00047: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6308 - val_loss: 0.7171\n",
            "Epoch 48/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6281\n",
            "Epoch 00048: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6281 - val_loss: 0.7165\n",
            "Epoch 49/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6252\n",
            "Epoch 00049: saving model to /content/drive/My Drive/lang/Checkpt_spa_g/cp.ckpt\n",
            "537/537 [==============================] - 32s 59ms/step - loss: 0.6252 - val_loss: 0.7130\n",
            "Epoch 50/50\n",
            "537/537 [==============================] - ETA: 0s - loss: 0.6221"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW7HB9E9XNV5"
      },
      "source": [
        "## Model 2 prediciton (Eng -> Other lang)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhHMXBeL6PFh"
      },
      "source": [
        "y_pred_logits_g = model_g.predict(X_test_g)\n",
        "\n",
        "y_pred_g=[]\n",
        "for pred_g in y_pred_logits_g: \n",
        "  y_pred_g.append(np.argmax(pred_g,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfuU6MO27nCA"
      },
      "source": [
        "y_pred_text_g = tokenizer_y_g.sequences_to_texts(y_pred_g)\n",
        "y_test_text_g = tokenizer_y_g.sequences_to_texts(y_test_g)\n",
        "X_test_text_g = tokenizer_X_g.sequences_to_texts(X_test_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6PJzlvC71QB",
        "outputId": "23281543-b3bb-4a57-8c9c-d34b3d114ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Original Sentence: \",X_test_text_g[256].replace(\"<unk>\",\"\").strip())\n",
        "print(\"Predicted Sentence: \",y_pred_text_g[256].replace(\"<unk>\",\"\").strip())\n",
        "print(\"Actual Sentence: \",y_test_text_g[256].replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence:  where's my mama\n",
            "Predicted Sentence:  माझी माझी कुठे आहे\n",
            "Actual Sentence:  माझी मम्मा कुठे आहे\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jraN0xIvTtCf"
      },
      "source": [
        "## Part 3: Testing on 5 examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILxK4Y_8BlN"
      },
      "source": [
        "five_example_sentence= np.array([X_test_g[120],X_test_g[-2],X_test_g[256],X_test_g[123],X_test_g[456]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD4LfA0ESbAZ",
        "outputId": "b20c4611-1a15-4082-80b6-6ea65efc1c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "five_example_sentence.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QQTz_LMTyX6"
      },
      "source": [
        "#### Prediction from Model 2 (Eng -> Chosen Lang)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJrr7t9qPxHQ"
      },
      "source": [
        "y_pred_logits_model2 = model_g.predict(five_example_sentence)\n",
        "\n",
        "y_pred_model2 =[]\n",
        "for pred_model2  in y_pred_logits_model2 : \n",
        "  y_pred_model2.append(np.argmax(pred_model2 ,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpiwYutPQW5J"
      },
      "source": [
        "y_pred_text_model2  = tokenizer_y_g.sequences_to_texts(y_pred_model2)\n",
        "y_pred_text_model2_cleaned =[]\n",
        "for predicted in y_pred_text_model2:\n",
        "  y_pred_text_model2_cleaned.append(predicted.replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbm4LPhOQklH",
        "outputId": "532dad63-5dea-47fb-a275-8ee9237a591e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(y_pred_text_model2_cleaned)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['टॉमला येऊ', 'मला माझी पुस्तक दे', 'माझी माझी कुठे आहे', 'टॉम काहीतरी होता होता', 'टॉम आणि मेरी दोघेही आहेत']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qRlME1iT5y3"
      },
      "source": [
        "#### Prediction from Model 1 (Chosen Lang -> Eng)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8FPisxeTGPz"
      },
      "source": [
        "model1_test_data_5 = pad_sequences(tokenizer_X.texts_to_sequences(y_pred_text_model2_cleaned),maxlen=X_test.shape[1],padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2M7FsGMQmcm"
      },
      "source": [
        "y_pred_logits_model1 = model.predict(np.array(model1_test_data_5))\n",
        "\n",
        "y_pred_model1=[]\n",
        "for pred_model1 in y_pred_logits_model1: \n",
        "  y_pred_model1.append(np.argmax(pred_model1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P28MfGDLSDzN"
      },
      "source": [
        "y_pred_text_model1 = tokenizer_y.sequences_to_texts(y_pred_model1)\n",
        "y_pred_text_model1_cleaned =[]\n",
        "for predicted_model1 in y_pred_text_model1:\n",
        "  y_pred_text_model1_cleaned.append(predicted_model1.replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYZGFI8WDRE"
      },
      "source": [
        "#### FINAL RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r7WxciEVnM3"
      },
      "source": [
        "original_example_sentence = tokenizer_y.sequences_to_texts(five_example_sentence)\n",
        "original_cleaned=[]\n",
        "for original in original_example_sentence:\n",
        "  original_cleaned.append(original.replace(\"<unk>\",\"\").strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOne-nZvS9ED",
        "outputId": "ba6c0431-c9dc-4b73-afa6-29f963745678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i in range(len(five_example_sentence)):\n",
        "  print(\"Original Sentence:\\t\\t\",original_cleaned[i])\n",
        "  print(\"Predicted Sentence By Model 2:\\t\",y_pred_text_model2_cleaned[i])\n",
        "  print(\"Predicted Sentence By Model 1:\\t\",y_pred_text_model1_cleaned[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence:\t\t let tom in\n",
            "Predicted Sentence By Model 2:\t टॉमला येऊ\n",
            "Predicted Sentence By Model 1:\t tom come come\n",
            "Original Sentence:\t\t give me my bag\n",
            "Predicted Sentence By Model 2:\t मला माझी पुस्तक दे\n",
            "Predicted Sentence By Model 1:\t give me the book\n",
            "Original Sentence:\t\t where's my mama\n",
            "Predicted Sentence By Model 2:\t माझी माझी कुठे आहे\n",
            "Predicted Sentence By Model 1:\t where's my tom\n",
            "Original Sentence:\t\t tom was hiding something\n",
            "Predicted Sentence By Model 2:\t टॉम काहीतरी होता होता\n",
            "Predicted Sentence By Model 1:\t tom was something\n",
            "Original Sentence:\t\t tom and mary both laughed\n",
            "Predicted Sentence By Model 2:\t टॉम आणि मेरी दोघेही आहेत\n",
            "Predicted Sentence By Model 1:\t tom and both both\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
